\begin{comment}
  \bibliography{project.bib}
\end{comment}

% drawing trees with latex:
% http://forcecore.tistory.com/1023#recentTrackback

\chapter{Huffman Coding}
\label{cha:huffman}

\section{History}

In 1951, professor Robert Fano gave his students in a course on
information theory two choices: to complete the course they would
either have to write a term paper or take a final exam. In the term
paper the students were asked to solve a seemingly simple problem. In
order to not discourage his students, Fano did not tell them that the
problem that he gave them was a problem that he and Claude Shannon,
the father of information theory, had been unable to
solve \cite{stix91:_profil}.

One of Fano's students was David Huffman. For months he tried to solve
the problem and he was in the end very close to giving up. But then,
in a moment of insight, he suddenly realized how to solve the
problem. He then published his solution to the rest of the world in
\cite{huf52}. The technique he described in this paper is known as
\textit{Huffman Coding}, and this is the compression method that we
will be describing in this chapter.

And luckily, Huffman decided not to patent the algorithm. And because
of this, Huffman Coding proved to be a very popular compression
technique. It is for example widely used in high-definition
televisions and modems.

\section{Code Lists}

The following description of Huffman Coding is mainly based on
Huffman's original description of the algorithm, \cite{huf52}, but
also on
\cite{Salomon:2004:DCC,mark1996data_compression_book,mcfadden92:_hackin_data_compr,mahoney11:_data_compr_explain,Lelewer:1987:DC:45072.45074,cormen2009introduction_to_algo}.

The key observation behind Huffman Coding is this: in any non-random
data there will almost always be some characters that are more
frequent than others. For example, in English text the by far most
common letter is ``e''
\cite{lewand2000cryptological,Shannon:2001:MTC:584091.584093}. So if a
random letter is picked from an English language text then it is most
likely that that letter will be an ``e''. So, what Huffman Coding does
is that it assigns more frequent characters to binary numbers with
less digits. These numbers are referred to as \textit{codes}. The
collection of all these computed codes, each assigned to a character,
will henceforth be known as the \textit{code list} of the input data.

\subsection{The Prefix Property}

Before we consider how the Huffman Algorithm chooses these codes, we
also have to consider what properties the code list should
satisfy. Are all possible code lists acceptable?

\begin{table}
  \centering
  \begin{tabular}{llll}
    \toprule
    Letter & Probability & Code List 1 & Code List 2 \\
    \midrule
    \texttt{A} & $0.35$ & \bin{01} & \bin{01} \\
    \texttt{B} & $0.35$ & \bin{11} & \bin{00} \\
    \texttt{C} & $0.15$ & \bin{001} & \bin{010} \\
    \texttt{D} & $0.15$ & \bin{000} & \bin{101} \\
    \bottomrule
  \end{tabular}
  \caption{Examples of code lists}
  \label{tab:codes-ex}
\end{table}

Let us say that we dealing with the four-length alphabet \texttt{A},
\texttt{B}, \texttt{C}, \texttt{D} and that for the sake of argument
the probabilities of these letters are the ones given in
table~\ref{tab:codes-ex}. In this table two possible code lists are
given. Let first consider the first of these code lists.  Character
that are more frequent are in this code list assigned shorter codes,
which is just the property that we are after. Using this code the
string \texttt{BABACD} would get encoded as

\begin{equation*}
  \bin{11}\ \bin{01}\ \bin{11}\ \bin{01}\ \bin{001}\ \bin{000}
\end{equation*}

This encoded can data can be decoded perfectly fine by translating the
codes to their corresponding letters. However, what would have
happened if we had used code list 2 instead?

\begin{equation*}
  \bin{00}\ \bin{01}\ \bin{00}\ \bin{01}\ \bin{010}\ \bin{101}
\end{equation*}

The first four codes can get decoded perfectly fine, but what about
the fifth? After reading the first two bits of this code we end up
with the code \bin{01}, which is the code for \texttt{A}. However, how
would the encoder know that it is supposded to read the next bit and
then parse it as the code for \texttt{C}? It can't. The fifth code in
combination with the sixth, \bin{010101} can be parsed as \texttt{CD},
but an equally valid way to decode this data would be as
\texttt{AAA}. So the second set of codes is \textit{ambiguous} while
the first is not. Since the encoded data can be parsed in several ways
using an ambiguous code, a file compressed using such a code is not
lossless in the first place!

The reason that the first code list is not ambiguous is because it has
the \textit{prefix property}. This means that no code in the code list
is the prefix of another code. On the other hand, in the code list 2,
the code \bin{01} is a prefix of the code \bin{010}, and therefore the
code list was ambiguous.

\begin{Exercise}[label={prefix-prop}]

  Which of the following code lists has the prefix property?  Also,
  suggest of a way to fix the code lists that do not obey by the
  prefix property.

  \begin{enumerate}[(a)]
  \item \bin{011}, \bin{0101},\bin{11},\bin{001}

  \item \bin{01}, \bin{00}, \bin{10}, \bin{101}

  \item \bin{1010101}, \bin{00001}, \bin{001}, \bin{1011}

  \end{enumerate}

\end{Exercise}

\section{Huffman Coding}

\subsection{Trees}

A tree is a recursive data structure that is used to implement Huffman
Coding. A tree consists of a collection of nodes with values, and
every node can have left and right child nodes. Two example trees:

\begin{huffmanc}
  \node (first)[hnode]{7};
  \node[hnode,right=of first,xshift=10mm]{2}
  child{node[hnode] {3}}
  child{node[hnode] {1}};
\end{huffmanc}

The first of these two trees is a single node with a value of $2$. It
has no left or right child nodes. The node with the value $2$ has two
child nodes. The left and right child nodes have the values $3$ and
$1$ respectively. You can also say that the node with the value $2$ is
\textit{parent} of the two nodes with the values $3$ and $1$.

\subsection{Computing the Code List}

Let us consider finding the Huffman codes for the letters of the
string \texttt{ababaacdd} for the alphabet \texttt{a}, \texttt{b},
\texttt{c}, \texttt{d}. First the letters and their respective
frequencies are assigned single nodes:

\begin{huffmanc}
  \firstcharnode{ctree}{1}{c}
  \restcharnode{btree}{ctree}{2}{b}
  \restcharnode{dtree}{btree}{2}{d}
  \restcharnode{atree}{dtree}{4}{a}
\end{huffmanc}

We now have forest of single nodes. The frequencies of the two nodes
with minimum frequencies are now summed. This sum is assigned to a new
node and this node is made the parent of the two former minimum
frequency nodes:

\begin{huffmanc}
  \node[hnode]{3}
  child{node(cnode)[hnode] {1}}
  child{node(bnode)[hnode] {2}};
  \nodechar{cnode}{c}
  \nodechar{bnode}{b}
  \restcharnode{bnode}{dtree,xshift=-30}{2}{d}
  \restcharnode{dtree}{atree}{4}{a}
\end{huffmanc}

Note that the order in which these two nodes are added does not
matter. It is perfectly acceptable to switch \texttt{b} and
\texttt{c}. Another alternative for the second minimum frequency node
to \texttt{b} was \texttt{d}, but neither this choice will affect the
validity of the resulting code list.

The last step is now repeated, meaning that the two minimum trees are
added to form an even bigger tree:

\begin{huffmanc}
  \node[hnode]{5}
  child{node[hnode]{3}
    child{node(cnode)[hnode] {1}}
    child{node(bnode)[hnode] {2}}}
  child{node (dnode) [hnode] {2}};
  \restcharnode{dtree}{atree,xshift=-80}{4}{a}

  \nodechar{cnode}{c}
  \nodechar{bnode}{b}
  \nodechar{dnode}{d}

\end{huffmanc}

And the same thing is done for the two remaining trees:

\begin{huffmanc}
  \node[hnode] {9}
  child {node[hnode]{5}
    child{node[hnode]{3}
      child{node(cnode)[hnode] {1}}
      child{node(bnode)[hnode] {2}}}
    child{node (dnode) [hnode] {2}}}
  child{node (anode) [hnode] {4}};

  \nodechar{cnode}{c}
  \nodechar{bnode}{b}
  \nodechar{dnode}{d}
  \nodechar{anode}{a}
\end{huffmanc}

The tree we end up with is known as the Huffman tree of the input
data. But it is important to realize that this is \textit{not} by any
means unique. This is because there will always be the possibility
that there are more than two trees with a minimum frequency.

To finally create the code list, the following is done: starting from
the root of the tree, it is followed along its branches to reach its
leaves(the bottommost nodes). If a left branch is followed then a
cleared bit($0$) is added to the resulting code, else a toggled
bit($1$) is added.

So to get the resulting code for \texttt{b}, we follow the tree from
its root to the leaf that has character \texttt{b}. To reach
\texttt{b} we go left, left and then right, so \texttt{b} has the code
\bin{001}. The final code list is given in table~\ref{tab:code-list}.

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    Letter & Code \\
    \midrule
    \texttt{a} & \bin{1} \\
    \texttt{b} & \bin{001} \\
    \texttt{c} & \bin{000} \\
    \texttt{d} & \bin{01} \\
    \bottomrule
  \end{tabular}
  \caption{Computed code list}
  \label{tab:code-list}
\end{table}

But what if the frequency of one the characters above would have been
0? Then that character would have been left out from the Huffman tree
computation, since there is no need to compute the Huffman codes for
characters that do not at all occur in the input data!

\begin{Exercise}[label={nuther}]
  Give an example of another valid Huffman that could have been
  constructed using the input data. What final code list does this
  tree instead result in?
\end{Exercise}

\subsection{Motivation}

Huffman Coding will in general always give valid a code list that
has the prefix property. A tree \textit{not} satisfying that property
would have to look something like this:

\begin{huffmanc}

  \node[hnode] {}
  child {node[hnode]{c}
    child{node[hnode]{b}}
    child{node (dnode) [hnode] {d}}}
  child{node (anode) [hnode] {a}};
\end{huffmanc}

Since the \texttt{c} node is parent of the nodes \texttt{b} and
\texttt{d}, its code will also be a prefix of the codes of \texttt{b}
and \texttt{d}. However, since only the leaves of the resulting tree
are assigned characters, it is impossible for such a tree to result
from Huffman Coding.

It is easy to see how the algorithm assigns shorter codes to more
frequent character. Since the most frequent characters will have
higher frequencies, they will also get added to the tree at the later
parts of the algorithm, and so they get placed higher in the tree, and
therefore they will get assigned the shorter codes. And similarly,
since the least occurring characters will get assigned small
frequencies, they will also get added into the tree early, so will get
placed deep down in the tree, and they will therefore get assigned the
longer codes.

\section{Answers to the exercises}

\begin{Answer}[ref={prefix-prop}]

  \begin{enumerate}[(a)]
  \item Has the prefix property.
  \item Does not have the prefix property. If the codes are on the
    other hand changed to \bin{01}, \bin{00}, \bin{101} and \bin{100}
    they will obey the prefix property.
  \item Has the prefix property.

  \end{enumerate}

\end{Answer}

\begin{Answer}[ref={nuther}]
  \begin{huffmanc}

    \node[hnode] {9}
    child {node[hnode]{5}
      child{node[hnode]{3}
        child{node(cnode)[hnode] {1}}
        child{node(bnode)[hnode] {2}}}
      child{node (dnode) [hnode] {2}}}
    child{node (anode) [hnode] {4}};

    \nodechar{cnode}{c}
    \nodechar{dnode}{b}
    \nodechar{bnode}{d}
    \nodechar{anode}{a}

  \end{huffmanc}

  This tree results in the codes \texttt{a}$=$\bin{1},
  \texttt{b}$=$\bin{01}, \texttt{c}$=$\bin{000} and
  \texttt{d}$=$\bin{001}.

\end{Answer}