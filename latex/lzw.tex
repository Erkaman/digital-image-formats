\begin{comment}
  \bibliography{project.bib}
\end{comment}

\chapter{LZW}
\label{cha:lzw}

In this chapter, we will discuss the data compression algorithm LZW,
as it is defined and described in
\cite{Nelson:1989:LDC:77102.77104,Welch:1984:THD:1319729.1320134,Salomon:2004:DCC,mark1996data_compression_book,nelson:_lzw_revis}. We
will first discuss the history behind the algorithm, then the
compression algorithm itself, after this we will describe the
decompression algorithm and in the end we will discuss the compression
efficiency of this algorithm.

\section{History}
\label{sec:hist-lzw}

In the years 1977 to 1978, the two Israeli researchers Abraham Lempel
and Jacob Ziv published two papers; in the first of these papers,
\cite{Ziv77auniversal}, they described the algorithm that came to be
known as LZ77(sometimes also known as LZ1) and in the second of these
papers, \cite{Ziv78compressionof}, they defined the algorithm that
would from then on become known as LZ78(also known as LZ2)
\cite{roelofs09:_histor_portab_networ_graph_png_format,Salomon:2004:DCC,winters:_us_paten_adapt}.

These two methods started the family of compression algorithms that is
known as the \textit{dictionary methods}. Examples of algorithms in
the family of dictionary methods is LZMA, which is based on LZ78
\cite{palov11}, LZW, which is also based on LZ78, and LZSS,which was
based on LZ77 \cite{Salomon:2004:DCC}. LZW is the method that we will
discuss in this chapter. It was invented by Terry Welch and he
published the algorithm in \cite{welch85:_u}.

\section{The Compression algorithm}
\label{sec:lzw-comp-desc}

The LZW compression algorithm is best
understood by walking through the LZW algorithm's process of
compressing some example string, so we will in this example
demonstrate how the compression of the string
$ababcbababaaaaaaa$ is done in LZW.

Central to the entire algorithm's process is a string table. In this
table, numbers called \textit{codes} are assigned to strings. The
algorithm starts by filling this string table with all the strings
that are possible to fit in a byte. So all the codes $0-255$ are
assigned their respective strings. We will for sake of simplicity use
ASCII encoding throughout this example, so the codes $0-255$ are
assigned their respective ASCII characters, as is shown in table
\ref{tab:str-tab-ascii}.

\newcommand{\dotsrow}{\dots & \dots \\}
\newcommand{\strrow}[2]{$#1$ & $#2$ \\}

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    Code & String\\
    \midrule

    \dotsrow
    \strrow{33}{!}
    \strrow{34}{"}
    \strrow{35}{\#}
    \dotsrow
    \strrow{97}{a}
    \strrow{98}{b}
    \strrow{99}{c}
    \strrow{100}{d}
    \strrow{101}{e}
    \dotsrow
    \strrow{255}{(\text{unassigned to in the ASCII set})}

    \bottomrule
  \end{tabular}
  \caption{The initial LZW string table, assuming that we are ASCII encoding.}
  \label{tab:str-tab-ascii}
\end{table}

\newcommand{\acode}{$a$\xspace}
\newcommand{\bcode}{$b$\xspace}
\newcommand{\ccode}{$c$\xspace}
\newcommand{\abcode}{$ab$\xspace}
\newcommand{\bacode}{$ba$\xspace}
\newcommand{\abccode}{$abc$\xspace}

Now the actual compression of the string starts. First it reads in the
character \acode. In the string table that letter has the code $97$,
so the code $97$ is outputted. Next, the character \bcode is read,
which has the code $98$. So $98$ is outputted and then the two strings
read so far are appended together to form the string \abcode. The
string table is searched for the string \abcode, but it cannot be
found in the string table, so it is added to the string table and is
assigned the next available code, $256$.

\acode is discarded, \bcode is kept and another \acode is
read. First \bcode is outputted, and the appended string
\bacode is formed. This string doesn't either exist in the table,
 so it is added to table and is given the code $257$.

 Discard \bcode, output \acode and read in another \bcode. The
 resulting appended string is this time \abcode. But wait a minute,
 isn't already that string in the string table? Yes it is, so instead
 of re-adding that string to table, it is kept for the next step in
 the algorithm.

The code for \abcode is outputted, which is $256$, and the character \ccode is
read. The string \abccode doesn't exist in the table, so it is
added to the string table and given the code $258$.

And the algorithm just keeps going on like this. Table
\ref{tab:str-tab-str} shows all the codes that were added during the
compression of the sample string $ababcbababaaaaaaa$. Table
\ref{tab:lzw-walkthru} gives a detailed walkthrough of the entire
compression process of that same string. So the string
$ababcbababaaaaaaa$ was in the end compressed down to the
string $ab,256,c,257,260,a,262,263,a$.

So to summarize, the LZW algorithm basically works like: the algorithm
keeps accumulating and reading characters into the string $S$. This
process goes on as long as, for the read in character $c$, the string
$Sc$, where $Sc$ represents the string that is formed when the
character $c$ is appended to the end of the string $S$, can be found
in the dictionary. When $Sc$, for some read in character $c$, cannot
be found in the dictionary, then the code for the string $S$ is
outputted, and the appended string $Sc$ is added to the dictionary.

\begin{Exercise}[label={lzw-compress}]

  LZW compress the string $abababab$. Show the output of the
  compressor and the resulting string table.

\end{Exercise}

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    String & Code \\
    \midrule
    \strrow{97}{a}
    \strrow{98}{b}
    \strrow{99}{c}
    \dotsrow
    \strrow{256}{ab}
    \strrow{257}{ba}
    \strrow{258}{abc}
    \strrow{259}{cb}
    \strrow{260}{bab}
    \strrow{261}{baba}
    \strrow{262}{aa}
    \strrow{263}{aaa}
    \strrow{264}{aaaa}
    \bottomrule
  \end{tabular}
  \caption{The resulting string table after the LZW algorithm has been run on
    the string $ababcbababaaaaaaa$.}
  \label{tab:str-tab-str}
\end{table}

\newcommand{\lzwrow}[6]{$#1$ = $#2$ & $#3$ = $#4$ &
  $#1$ = $#2$ & $#5$ = $#6$ \\}

\newcommand{\stoplzwrow}[2]{$#1$ = $#2$ & STOP &
  $#1$ = $#2$ & STOP \\}

\begin{table}
  \centering
  \begin{tabular}{llll}
    \toprule
    String Code & Character Code & Output Code & New table entry \\
    \midrule
    \lzwrow{a}{97}{b}{98}{ab}{256}
    \lzwrow{b}{98}{a}{97}{ba}{257}
    \lzwrow{ab}{256}{c}{99}{abc}{258}
    \lzwrow{c}{99}{b}{98}{cb}{259}
    \lzwrow{ba}{257}{b}{98}{bab}{260}
    \lzwrow{bab}{260}{a}{97}{baba}{261}
    \lzwrow{a}{97}{a}{97}{aa}{262}
    \lzwrow{aa}{262}{a}{97}{aaa}{263}
    \lzwrow{aaa}{263}{a}{97}{aaaa}{264}
    \stoplzwrow{a}{97}
    \bottomrule
  \end{tabular}
  \caption{The LZW compression process on the string $ababcbababaaaaaaa$.}
  \label{tab:lzw-walkthru}
\end{table}

\section{Technical Implementation of The Compression Algorithm}
\label{sec:lzw-enc-algorithm}

In algorithm \ref{alg:lzw-compression} the complete LZW compression
algorithm is presented. Try reading through it at least once, but I
doubt you'd be able to understand much of it at this point. The
implementation of the algorithm turns out not to be as easy as it may
seem

\begin{algorithm}[H]
  \caption{The LZW compression algorithm.}
  \label{alg:lzw-compression}
  \begin{algorithmic}[1]

    \Let{$maxValue$}{$2^{codeSize} - 1$}
    \Let{$maxCode$}{$maxValue - 1$}

    \State Fill the string table for the codes $1-255$
    \Let{$nextCode$}{$256$} \Comment{The code value the next string
      will be assigned to.}
    \Let{$string$}{\VoidCall{$ReadByte$}}
    \Let{$character$}{\VoidCall{$ReadByte$}}

    \While{\neof}

      \If{\Call{InStringTable}{$string + character$}} \label{algl:hasingcheckintable}
        \linecomment{Keep accumulating the string}
        \Let{$string$}{$string + character$}

      \Else

        \State \Call{outputCode}{$string$}

        \linecomment{Only add the accumulated string to the table if
          there's space for it.}  \If{$nextCode \leq maxCode$}

          \State \Call{AddToStringTable}{$nextCode, string + character$}\label{algl:hashadd}
          \Inc{nextCode}{1}

        \EndIf

        \Let{$string$}{$character$}

      \EndIf

      \Let{$character$}{\VoidCall{$ReadByte$}}

    \EndWhile

    \State \Call{outputCode}{$string$}
    \State \Call{outputCode}{$maxValue$}
  \end{algorithmic}
\end{algorithm}

\subsection{Codes sizes}

All a LZW compressed file will consist of after this algorithm has
been run on it, is just a long sequence of outputted codes.  Since all
the of the string codes added during the compression step will have
code values over $255$, bytes will obviously not be sufficient to
store these codes. In our version of the LZW algorithm, fixed code
sizes are assigned to every code. So this means that the codes are
stored in fixed $n$-bit numbers for some value $n$. The most typically
used code sizes are in the range $9 \leq n \leq 15$, and this is the
also the range of code sizes that that the sample compressor for this
chapter supports.

This is of course not the only possible storage model for the
codes. In the GIF format's variation of the LZW algorithm, increasing
codes sizes are used to even further improve the compression
efficiency of this algorithm. We'll discuss this variation in chapter
\ref{cha:gif}.

\subsection{Knowing when to stop}

The compressor has to be notified of when it is done compressing the
data. The largest possible value of a valid code size is therefore
outputted in the end of the algorithm.

\subsection{Table size}

Since we are using fixed code sizes there must be a limit on how large
the string table can grow. For any fixed code size $n$, this code size
can represent $2^n$ codes, and therefore the maximum length for the
string table will also be $2^n$. For 12-bit codes the maximum length
of the string table is $2^{12}=4096$.

There is also another issue that we will need to discuss; how will the
decompressor know when the end of the compressed data has been
reached? Well, wouldn't it obviously know that when it has reached to
the end of the file? That is one way of doing it, but many LZW
compressors use another method: in these compressors, at the end the
compressed data the maximum possible code value is outputted. For the
code-size $n$ bits this is the value $2^n - 1$.

But this has another important implication; since one code is
reserved, the actual number of strings that the string table can store
will be $2^n - 1$. And furthermore, the maximum code that can be
assigned to a string will be $2^n - 2$, and not $2^n - 1$.

\subsubsection{Packing order}

When, for example, the codes $1011$ and $1100$ are to be packed, there
can actually be two different resulting byte packets: $1011\ 1100$ or
$1100\ 1011$. In the first case, the bits of the codes start to get
packed from the \textit{most significant bits} to the \textit{least
  significant bits} of the resulting packet. In the second case, the
bits from the codes get packed from the \textit{least significant
  bits} to the \textit{most significant bits} of the packet.

The first of these two packing orders we'll from now on call
MSBF, which stands for \textbf{M}ost \textbf{S}ignificant
\textbf{B}its \textbf{F}irst and the second order we'll refer to as
LSBF, which stands for \textbf{L}east \textbf{S}ignificant
\textbf{B}its \textbf{F}irst.

It is important that differ between these two orders, \textit{because
  different packing orders are used in different formats}. In the GIF
format the LSBF order is used, while in other implementation MSBF is
instead used. The GIF format wile be the main
subject of chapter
\ref{cha:gif}.

\begin{Exercise}[label={bit-packing-order}]

  What packets would get outputted if the codes that we wanted to
  output were $0110$, $1100$, $0001$ and $1111$, if the packing order
  was

  \begin{enumerate}[(a)]
  \item MSBF
  \item LSBF
  \end{enumerate}

\end{Exercise}

\section{Description of the Decompression algorithm}

Using the compression algorithm LZW on the string $ababcbababaaaaaaa$
produced the compressed data $ab, 256,c, 257, 260, a, 262, 263, 264,
a,4095$. Since we were using 12-bit codes to encode the data, the
number $2^{12} - 1 = 4095$ terminates the data. This number is used by
the decompressor to check if it is done with the decompression.

In a LZW compressed file the string table used to encode the file is
not included. But how could you possibly decompress the file without
such essential information? It turns out that only from the compressed
data the decompressor is able to rebuild an exact copy of the string
table built during the compression.

We wanted to decompress $ab, 256,c, 257, 260, a, 262, 263, 264,
a,4095$. The decompressor first assign the code values $0-255$ to
their corresponding single length strings, and makes the same table
\ref{tab:str-tab-ascii} that was also built in the beginning of the
compression.

The characters $a$ and $b$ are read and $a$ is outputted. Now $a$ and
$b$ are appended together to form the string $ab$. This string is
added to the string table and is given the code $256$.

A new code $256$ is read, $a$ is discarded and $b$ is outputted. The
code $256$ is according to our current string table the string
$ab$.

Now we need to form the next string to be added to the table. To do
this, we take the string $b$ and append it to the first character of
the string $ab$, thus forming the string $ba$. This string is added to
the table with the code $257$.

The character is read $c$, the old $b$ is discarded and the string
$ab$ is now outputted. $ab$ and $c$ are appended together to form the
string $abc$(The first character of the one length string $c$ is
simply $c$) and then $abc$ is added to the string table with
the code $258$.

The code $257$ is read in, which corresponds to the string $ba$. Since
the former code was $c$, the string to be added to the table is $cb$,
and this string is given the code $259$.

Next the code $260$ is read. But we haven't even yet added that code
to the table! How could this be?

Up to this point, the compressor had compressed the string $ababc$ and
had defined the code for the string $ba$ as $257$. What remained to be
compressed after this was $bababaaaaaaa$. So it read the string $ba$
and the character $b$, becuase $ba$ was already defined in the string
table. After outputting the code for $ba$, it added the string $bab$
to string the string table, assigning it the code $260$. Then it threw
away $ba$ and kept $b$. Then it accumulated the string $bab$ until it
found the character $a$. At this point, the code $260$ for the string
$bab$ was outputted.  The important thing to realize here is that this
happened before the decompressor even had the chance the define the
code $260$.

\newcommand{\ko}{\ensuremath{k\omega}\xspace}
\newcommand{\kok}{\ensuremath{\ko k}\xspace}
\newcommand{\kokok}{\ensuremath{\kok \omega k}\xspace}

So the general problem could described like this\cite{welch85:_u}: if
the string \ko is already in the string table and if the string \kokok
is encountered, the compressor will first add \kok the string table,
and then output \ko. It will after this accumulate and output the code
for the string \kok, and this is before the decompressor has defined
that code in its own string table. However, since this is the only
special case in which a undefined code can legally be found, this
special case can be handled by instead defining the current string to
be $\kok$. This string is equivalent to the string represented by the
undefined code.

Now let us return to our example string. The codes we currently had at
hand were $ba$ and $260$. We want to get the characters $k$ and
$\omega$, so that we may construct the string $\kok$. Since $ba$ are
the first two letters of the string $\kokok$, then $k = b$ and $\omega
= a$, so the string represented by the $260$ is $\kok = bab$.

And the decompressor just keeps working through the data like this,
while at the same time reconstructing the string table that was used
to compress the data in the first place. This just keeps on going
until it encounters the maximum value of the current code size $n$,
$2^{n}-1$. For 12-bit codes this value is $2^{12} - 1 = 4095$, as it
can be seen from our example data $97, 98, 256, 99, 257, 260, 97, 262,
263, 264, 4095$.

\begin{Exercise}[label={lzw-decompress}]

  Assuming that 12-bit codes were in the compression of exercise
  \ref{lzw-compress}, then compress the resulting data
  $97,98,256,258,98,4095$.

\end{Exercise}

In \ref{alg:lzw-working-decompression} pseudocode of the algorithm we
just described is given.

\begin{algorithm}[H]
  \caption{LZW working decompression algorithm.}
  \label{alg:lzw-working-decompression}
  \begin{algorithmic}[1]

    \State \VoidCall{FillInitialStringTable}
    \Let{$nextCode$}{$256$}

    \Let{$oldCode$}{\VoidCall{$InputCode$}}
    \State \Call{writeByte}{$oldCode$}

    \Let{$character$}{$oldCode$}

    \Let{$newCode$}{\VoidCall{$InputCode$}}

    \While{$newCode \neq maxValue$}

      \If{$\NOT$ \Call{IsInTable}{$newCode$}} \Comment{The special case}
        \Let{$string$}{\Call{TranslateCodeToString}{oldCode}}
        \Let{$string$}{$string + character$}
      \Else
        \Let{$string$}{\Call{TranslateCodeToString}{$newCode$}}
      \EndIf

      \State \Call{outputString}{$string$}

      \Let{$character$}{$string[0]$} \Comment{Get the first character
        of the string.}

      \If{$nextCode \leq maxCode$}
        \linecomment{Add the translation of $oldCode$ $+$ $character$
          to the table}
        \State \Call{$AddToStringTable$}{$nextCode,oldCode, character$}

        \Inc{nextCode}{1}
      \EndIf

      \Let{$oldCode$}{$newCode$}

      \Let{$newCode$}{\VoidCall{$ReadByte$}}

    \EndWhile
  \end{algorithmic}
\end{algorithm}

\section{Compression Efficiency Of LZW}

In this section, we will be showing and discussing the results of a
test that tested how the compression ratio of the LZW algorithm varied
for different sorts of files and data with different codes sizes.

\subsection{How compression is achieved in LZW}

The main idea behind LZW compression is that strings that occur often
within a file will get replaced by shorter codes. And in general, the
longer the algorithm runs, the longer the strings that get added to
the table get. This has the consequence that larger files typically
have a much better compression ratio than smaller files.

\subsection{The Canterbury Corpus}

Here we introduce a set of test files that we will be using to test
the efficiency of compression algorithms. A corpus of files designed
for testing new compression algorithms called the Canterbury Corpus is
that we will be using. These files were selected by
\cite{arnold:corpus} out of thousands of other files because they were
shown to test the efficiency of lossless compression algorithms the
best. Table \ref{tab:corp-files} lists all the files in the Canterbury
Corpus. They can all be downloaded at \cite{powell:desc-corp}.

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    File & Category \\
    \midrule
    alice29.txt & English text \\
    asyoulik.txt & Shakespeare Play \\
    cp.html & HTML source \\
    fields.c & C source \\
    grammar.lsp & LISP source \\
    kennedy.xls & Excel Spreadsheet \\
    lcet10.txt & Technical writing \\
    plrabn12.txt & Poetry \\
    sum & SPARC Executable \\
    xargs.1 & GNU manual page \\
    \bottomrule

  \end{tabular}
  \caption{The Canterbury Corpus.}
  \label{tab:corp-files}
\end{table}

\subsection{The Test}

The purpose of the test was to test for the compression ratio for all
the files in table \ref{tab:corp-files} for different code fixed sizes
$n$ in the range $9 \leq n \leq 15$.

The program that was used to do the compression was \verb|LZW|, which
also is the program that was written to demonstrate the techniques
discussed in this chapter. The source code for the program can be find
in \verb|code/lzw/|.

To run the run the program on all the test files and accumulate the
gathered data into a \LaTeX{} table, a Python\footnote{\url{http://python.org/}} script was written. This
script can be found at the location \verb|code/lzw/test.py|.

\subsection{The Test Results}

In table \ref{tab:lzw-test-results} we show the results of the test.

\begin{table}
  \scriptsize
  \centering
  \input{../code/lzw/table.tex}
  \caption{LZW compression test results. The percentages are compression ratios.}
  \label{tab:lzw-test-results}
\end{table}

\subsection{Discussion}

Now we will discuss the results gathered in table
\ref{tab:lzw-test-results} and see if we can draw any sort of
conclusion from them.

\subsubsection{Human Readable Text}

We will first discuss the four files \verb|asyoulik.txt|,
\verb|alice29.txt|, \verb|plrabn12.txt|, \verb|lcet10.txt|. What these
files have in common is that they all contain only human-readable
English text. Interestingly enough, if we exclude the file
\verb|asyoulik.txt|, they all seem to share a common pattern: larger
codes sizes means better compression(the lower compression ratio, the
better compression). This makes sense because the English language,
and human languages in general, is full of redundancies. So for larger
code sizes, larger string tables can be built and therefore a greater
number of redundancies can be eliminated from the text.

You may also notice that the largest files, \verb|plrabn12.txt| and
\verb|lcet10.txt|, achieved the best compression. This is because for
longer files much more of the string table will get built up. The
longer a compression goes on, the larger the string table gets. And
the larger the string table gets, the better the data can be
compressed.

For the smallest file, \verb|asyoulik.txt|, the best compression ratio
was not achieved for the largest code size. This is because for
smaller files it is unlikely that the string table even gets fully
built up. If the size of the code doesn't get used to its full
potential, then unnecessary bits are only added when the code size is
increased. These unnecessary bits has the consequence that the codes
are stored in larger numbers, and this in turn can unnecessarily
increase the size and worsen the compression.

\subsubsection{Program code}

For the two source code files, \verb|fields.c| and \verb|grammar.lsp|,
the compression ratio doesn't get very good at all, compared to the
compression ratio of the text files. This is surprising, because
source code tends to contain tons of keywords, like
\verb|for|,\verb|while| \verb|return| that are repeated throughout the
entire file. You'd think that by replacing all these repeated words by
small codes this would result a good compression ratio.

But the worse compression ratio for higher codes sizes is in this case
attributed to the very low size of the files. Since these files are so
small, it is very hard for them to get a better compression ratio than
that of the huge English text files.

\subsubsection{Both text and code}

The files \verb|cp.html| and \verb|xargs.1| could be said to contain a
combination of both English human-readable text and source
code. \verb|xargs.1| is a pretty small file and hence gets its maximum
compression ratio for the rather small code size of $11$. The file
\verb|cp.html| is bit larger than the former, but the compression
ratio for this file behaves in largely the same way.

\subsubsection{Binary formats}

The files \verb|sum| and \verb|kennedy.xls| are files in binary
format. That is to say, they consist of a sequence of numbers
unreadable by humans. Binary files can most often only be interpreted
by the applications that created them. Image formats are examples of
binary formats.

The file \verb|kennedy.xls| achieves the best possible compression
ratio in this entire test. This is mainly because this is the largest
file in the entire Canterbury Corpus.

\verb|sum| is a binary file that has very unstable and unpredictable
compression ratios. But it in general seems to achieve the better
ratios using the larger codes.

\subsection{Conclusion}

The main conclusion of these tests is that for larger files larger
code sizes also tend to result in better compression ratios. For
smaller files larger codes can on the other hand unnecessarily worsen
the compression ratio.

\FloatBarrier

\section{Answers The Exercises}

\begin{Answer}[ref={lzw-compress}]

%\texttt{abababab}

  \begin{center}
    \begin{tabular}{llll}
      \toprule
      String Code & Character Code & Output Code & New table entry \\
      \midrule

      \lzwrow{a}{97}{b}{98}{ab}{256}
      \lzwrow{b}{98}{a}{97}{ba}{257}
      \lzwrow{ab}{256}{a}{97}{aba}{258}
      \lzwrow{aba}{258}{b}{98}{abab}{259}
      \stoplzwrow{b}{98}
      \bottomrule
   \end{tabular}


  \end{center}

  So the string is compressed down to $97,98,256,258,98$.
\end{Answer}

\begin{Answer}[ref={bit-packing-order}]

  \begin{enumerate}[a]
  \item $0110\ 1100$ and $0001\ 1111$.
  \item $1100 \ 0110$ and $1111 \ 0001$
  \end{enumerate}

\end{Answer}

\begin{Answer}[ref={lzw-decompress}]

  The decompressed data you should get is $abababab$.

\end{Answer}