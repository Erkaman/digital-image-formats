\begin{comment}
  \bibliography{project.bib}
\end{comment}

\chapter{LZW}
\label{ch:rle}

\begin{refsection}

  In this chapter, we will discuss the data compression algorithm
  \lzw\index{LZW} as it is described in
  \cite{nelson89:_lzw_data_compr,Welch:1984:THD:1319729.1320134,Salomon:2004:DCC}. We
  will first discuss the history behind the algorithm, then
  compression algorithm\index{compression algorithm}, after this the
  \textit{de}compression algorithm and in the end we will discuss the
  efficiency and compression ratio of this algorithm.

\section{(short)History}

\lzw stands for \textbf{L}empel, \textbf{Z}iv and
\textbf{W}elch. Lempel and Ziv are the two researches who developed
the two compression methods \lzseven and \lzeight. These two
algorithms became the basis for many other \lz variations including
\lzma \cite{palov11}, \lzw and \lzss \cite{Salomon:2004:DCC}. But we
will only discuss \lzw in this chapter. And finally, Welch is the guy
the developed this \lz variation.

\section{The Compression algorithm}

\subsection{Description}
\label{sec:lzw-desc}

The \lzw compression\index{\lzw compression} algorithm is best
understood by walking through the LZW algorithm's process of
compressing some sample data.

The algorithm is going to compress the sample data
\texttt{ababcbababaaaaaaa}.

Central to entire algorithm's process is a string table. In this
table, number called \textit{code}\index{code} is assigned to a
string. The algorithm start by filling the string table with all the
strings that are possible to fit in a byte. That is, it adds all the
possible number codes 0-255 to the string table and these number codes
are assigned a one length string that is the character that it
represents in the current encoding.. For this example we will use the
character set \ascii \cite{rfc20}, but really any character set could
be used. But in fact it is not really necessary that character set is
used at all, because all these strings are supposed to represent

Table \ref{tab:str-tab-ascii} shows how this table ends up looking
like when it is filled.

\newcommand{\dotsrow}{\dots & \dots \\}
\newcommand{\strrow}[2]{$#1$ & #2 \\}

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    Code & String\\
    \midrule

    \dotsrow
    \strrow{33}{!}
    \strrow{34}{"}
    \strrow{35}{\#}
    \dotsrow
    \strrow{97}{a}
    \strrow{98}{b}
    \strrow{99}{c}
    \strrow{100}{d}
    \strrow{101}{e}
    \dotsrow
    \strrow{255}{}

    \bottomrule
  \end{tabular}
  \caption{The initial LZW string table, filled with all the \ascii
    characters.}
  \label{tab:str-tab-ascii}
\end{table}

%\newcommand*{\newcode}[1]{%
%  \expandafter\newcommand\csname#1\endcsname c[1][]{\texttt{#1}##1\xspace}}

%\newcode{a}
\newcommand{\acode}{\texttt{a}\xspace}
\newcommand{\bcode}{\texttt{b}\xspace}
\newcommand{\ccode}{\texttt{c}\xspace}
\newcommand{\abcode}{\texttt{ab}\xspace}
\newcommand{\bacode}{\texttt{ba}\xspace}
\newcommand{\abccode}{\texttt{abc}\xspace}

Now the algorithm actually start going through the sample
string. First it reads in the character \acode. In the string table
that letter has the code $97$, so the code $97$ is outputted. Next,
the character \bcode is read, which has the code $98$. So $98$ is
outputted and then the two strings read so far are appended together
to form the string \abcode. The string table is searched for the
string \abcode, but it cannot be found in the string table, so it is
added to the string table and is assigned the next available code, $256$.

\acode is discarded, \bcode is kept and another \acode is
read. First \bcode is outputted, and the appended string
\bacode is formed. This string doesn't either exist in the table,
 so it is added to table and is given the code $257$.

Discard \bcode, output \acode and read in another
\bcode. Now something very interesting happens; the
resulting appended string is this time \abcode. But wait a minute,
isn't already that string in the string table? Yes it is, so instead
of re-adding that string to table, it is kept for the next step in the
algorithm.

The code for \abcode is outputted, which is $256$, and the character \ccode is
read. The string \abccode doesn't exist in the table, so it is
added to the string table and given the code $258$.

And the algorithm just keeps going like this. Table
\ref{tab:str-tab-str} shows all the codes that were added during the
compression of the sample string. Table \ref{tab:lzw-walkthru} gives a
detailed walkthrough of the compression of the same string. Do study
both of these tables table carefully and don't continue reading until you
fully understand what happened during the compression of the
string.

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    String & Code \\
    \midrule

    \dotsrow
    \strrow{256}{ab}
    \strrow{257}{ba}
    \strrow{258}{abc}
    \strrow{259}{cb}
    \strrow{260}{bab}
    \strrow{261}{baba}
    \strrow{262}{aa}
    \strrow{263}{aaa}
    \strrow{264}{aaaa}
    \bottomrule
  \end{tabular}
  \caption{The string table after the \lzw algorithm has been run on
    the string \texttt{ababcbababaaaaaaa}.}
  \label{tab:str-tab-str}
\end{table}

\begin{table}
  \centering
  \newcommand{\lzwrow}[4]{#1 & #2 & #3 & #4 \\}
  \begin{tabular}{llll}
    \toprule
    String Code & New Code & Output Code & New table entry \\
    \midrule

    \lzwrow{a}{b}{a = 97}{ab = 256}
    \lzwrow{b}{a}{b = 98}{ba = 257}
    \lzwrow{a}{b}{}{}
    \lzwrow{ab}{c}{ab = 256}{abc = 258}
    \lzwrow{c}{b}{c = 99}{cb = 259}
    \lzwrow{b}{a}{}{}
    \lzwrow{ba}{b}{ba = 257}{bab = 260}
    \lzwrow{b}{a}{}{}
    \lzwrow{ba}{b}{}{}
    \lzwrow{bab}{a}{bab = 260}{baba = 260}
    \lzwrow{a}{a}{a = 97}{aa = 261}
    \lzwrow{a}{a}{}{}
    \lzwrow{aa}{a}{aa = 262}{aaa = 262}
    \lzwrow{a}{a}{}{}
    \lzwrow{aa}{a}{}{}
    \lzwrow{aaa}{a}{aaa = 263}{aaaa = 263}

    \bottomrule
  \end{tabular}
  \caption{Detailed \lzw compression on the string \texttt{ababcbababaaaaaaa}.}
  \label{tab:lzw-walkthru}
\end{table}

\subsection{Algorithm}

In algorithm \ref{alg:lzw-compression} the complete LZW compression
algorithm is presented. Try reading through it at least once,
but I doubt you'd be able to understand all of it at this point. The
implementation of the algorithm turns out not to be as easy as it
sounds.

\begin{algorithm}[H]
  \caption{The LZW compression algorithm.}
  \label{alg:lzw-compression}
  \begin{algorithmic}[1]
    \Let{$nextCode$}{$256$}
    \Let{$stringCode$}{\VoidCall{$ReadByte$}}
    \Let{$charCode$}{\VoidCall{$ReadByte$}}

    \While{\neof}

      \If{\Call{InStringTable}{$stringCode,charCode$}} \label{algl:hasingcheckintable}
        \Let{$stringCode$}{\Call{GetCode}{$stringCode,charCode$}} \label{algl:hasgetcode}
      \Else
        \State \Call{outputCode}{$stringCode$}

        \If{$nextCode \leq maxCode$}

          \State \Call{AddToStringTable}{$nextCode, stringCode, charCode$}\label{algl:hashadd}
          \Let{$nextCode$}{$nextCode + 1$}

        \EndIf

        \Let{$stringCode$}{$charCode$}

      \EndIf

      \Let{$charCode$}{\VoidCall{$ReadByte$}}

    \EndWhile

    \State \Call{outputCode}{$stringCode$}
    \State \Call{outputCode}{$maxValue$}

  \end{algorithmic}
\end{algorithm}

\subsubsection{Codes sizes}

All a \lzw compressed file will consist of after this algorithm has
been run, is just a long sequence of outputted codes.

Since all the of the actual strings(whose length is greater than one)
in the string table have codes values over $255$, bytes will obviously
not be sufficient to store these codes. The code sizes\index{code
  size} used in the \lzw algorithm are typically in the ranges $9 \leq
codeSize \leq 15$. But in our examples we will be using 12-bit codes,
as they are most commonly used.

File I/O Functions for writing data to files are not typically
designed to write numbers whose bit-count are not multiples of 8, so
we will need to construct a function, an algorithm, for doing this. It
should at least support writing numbers in the ranges  $9 \leq x \leq 15$.

\todo{discuss the function outputCode here?}

\subsubsection{String Storage Model}

\newcommand{\strpair}[2]{(#1,#2)}

You could of course just store all the strings in the string table as
real strings. However, these strings are going to end up
taking way more space than they really need to.

As you probably remember from section \ref{sec:lzw-desc}, the string
to be added to the table is just the concatenation of the character
just read and the former string. The concatenation of the characters
characters \texttt{a} and \texttt{b} could be expressed as the string
\texttt{ab}. But however, they could also be described by the
pair\index{pair} \strpair{97}{98}, which consists of the two codes of
the two strings. Okay, there is no real big difference in size yet,
but now consider \texttt{abcbde} and \texttt{a} and their
concatenation \texttt{abcbdea}. If \texttt{abcbde} has the code, say,
289, then surely the pair \strpair{289}{97} is much more space
efficent than \texttt{abcbdea}?  Yes it is, of course!

You may think we are just nitpicking here, but also need to consider
that with huge files this string table could end up becoming huge. And
besides, pair of numbers are often much easier to deal with than
strings, espically in languages like \C!

Using this new string storage model table \ref{tab:str-tab-str}, is
translated to table \ref{tab:str-tab-pair}. You also may notice that we end
creating this beautiful recursive structure using this model. We will
find out more about this later when are going to decompress the data.

\newcommand{\pairrow}[3]{$#1$ & \strpair{#2}{#3} \\}

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    String & Code \\
    \midrule
    \dotsrow
    \pairrow{256}{97}{b}
    \pairrow{257}{98}{a}
    \pairrow{258}{256}{c}
    \pairrow{259}{99}{b}
    \pairrow{260}{257}{b}
    \pairrow{261}{260}{a}
    \pairrow{262}{87}{a}
    \pairrow{263}{262}{a}
    \pairrow{264}{263}{a}
    \bottomrule
  \end{tabular}
  \caption{The string table after the \lzw algorithm has been run.}
  \label{tab:str-tab-pair}
\end{table}

\subsubsection{Table size}

Since we are using fixed code sizes there is a limit on how large the
string table can grow. For example, for 12-bit codes the maximum size
of the string table will be $2^{11}=4096$. In algorithm
\ref{alg:lzw-compression} the constants $maxCode$ and $maxValue$ are
used to be make sure that the table doesn't grow beyond it's maximum
size. In algorithm \ref{alg:lzw-constants} we show how set the size
constants for the example code size 12. Notice that the maximum value
of the final code, $maxCode$ is one less than the actual max size of
the table. That's because at the end of algorithm we output the final
code to always be the maximum value of the current code size. You will
soon see why we do this when discuss the decompression algorithm, but for
now ignore it.

\begin{algorithm}[H]
  \caption{Settings the constants for the LZW algorithm for the
    example code size 12.}
  \label{alg:lzw-constants}
  \begin{algorithmic}[1]
    \Let{$codeSize$}{12}
    \Let{$maxValue$}{$(1 << codeSize) - 1$}
    \Let{$maxCode$}{$maxValue - 1$}
  \end{algorithmic}
\end{algorithm}

\subsubsection{Managing The String Table}

Please Inspect line
\algref{alg:lzw-compression}{algl:hasingcheckintable} and line
\algref{alg:lzw-compression}{algl:hasgetcode} of the compression
algorithm. Since we are clearly using the strings themselves to search
for their corresponding codes, we need to figure out how to find their
respective codes.

One way to implement this would be by using the code as an index to an
array of strings. But this clearly prohibitively slow as you would
have search the entire string table, string by string, when you would
want the code of a specific string.

The easiest way to solve this problem is to just use a hash table. The
string can used to create a key to index the table to get the searched
codes.

Since most modern languages already implement some sort of hash table
in their standard library, this functionality should not be very hard
to implement. If you are using more lower level language like \C, you
will either have to roll your own or use a library. In the sample \C
source code of this chapter a simple hand-rolled hash table
is used to manage the string table.

\section{Decompression algorithm}

\subsection{Description}

Using the compression algorithm LZW we on the string
\texttt{ababcbababaaaaaaa} we end up with the compressed data stream
\texttt{97 98 256 99 257 260 97 262 263 264 4095}. We were using
12-bit codes to encode the data, and so the maximum value of an
unsigned 12-bit number is used to terminate it. This is to make sure
that the decompressor knows when to stop reading the data.

As you may have noticed, in a LZW compressed file the string table is
not included. Then how could you possibly decompress the file without
such essential information? It turns out that all the corresponding
LZW decompression algorithm do is basically the same as the
compression aglorithm do; it just constructs the string table as it
goes read and compresses the file at the same time.

It is actually very simple; So, we wanted to decompress the string
\texttt{97 98 256 99 257 260 97 262 263 264 4095}. The decompressor
first reads the first code, $97$, which is just the letter a and the
letter a is outputted. Then the next code is read in, $98$, or simply
b. The first characater is the second string is b. And so the former
character a and the first string of the current string are appended
together and then added to the table, creating the very first table
entry, \strpair{97}{98} and it is assigned the code $256$. A new code,
256 is read and the old \texttt{a} is discarded. $256$ is now to be
translated. We just assigned the code $256$ the string \texttt{ab}, so
the translation of $256$ is obvious \texttt{ab}. The first character
of the string \texttt{ab} is \texttt{a} and thus the old string b and
the first characer of the new string \texttt{a} are added
together. And so ths second string table entry was constructed, and
given the value $257$.

The decompressor jsut keeps working through the data like this while
deconstructing the string table that was used to compress it. It reads
until it encounters the maximum value of the current code size $x$,
$2^{x}-1$; for 12-bit codes this $2^{12} - 1 = 4095$, as can be seen
from out test data stream, \texttt{97 98 256 99 257 260 97 262 263 264
  4095}.

\subsection{Almost working Algorithm}

And the algorithm we just described can be seen in algorithm
\ref{alg:lzw-non-working-decompression}. Having discussed the
compression algorithm very much in depth, you should be able to
perfectly understand must of this alglorithm. However, there is still
some more things we need to discuss until we are done with the \lzw
algorithm. Do notice that I say that algorithm is non-working. I still
present  to you the non-working algorithm because it works a majority
of the time and it is much easier to explain. In the next section, we
will present the working LZW decompression algorithm.

\begin{algorithm}[H]
  \caption{LZW non-working decompression algorithm.}
  \label{alg:lzw-non-working-decompression}
  \begin{algorithmic}[1]
    \Let{$oldCode$}{\VoidCall{$InputCode$}}
    \State \Call{WriteByte}{$oldCode$}

    \Let{$newCode$}{\VoidCall{$ReadByte$}}
    \While{$newCode \neq maxValue$}

      \Let{$string$}{\Call{translate}{$newCode$}}
      \State \Call{outputString}{$string$}
      \Let{$character$}{$string[0]$} \Comment{Get the first character
        of the translated string.}

      \State \Call{$AddToStringTable$}{$oldCode, character$}

      \Let{$oldCode$}{$newCode$}

      \Let{$newCode$}{\VoidCall{$ReadByte$}}
    \EndWhile
  \end{algorithmic}
\end{algorithm}

\subsubsection{Translating string codes}

Figuring out how to translate string codes of the code pair format as we
previously discussed. See algorithm
\ref{alg:translate-string-code}. This puts the decoded string of the
code $code$ \textit{reversed} into the string $decodedString$. The
stirng ends up being reversed because we are tranlating the code from
the end of the string to the beginning. As you can see, we are just
working thourgh character code after character code looking up the
next code in the table using the string code. For this algorithm, we
have chosen to put negative one into pairs that represent single
characters. If we have reached such a pair, we have reached to
beginning of the string and we are done translating it.

This discussing leads up to another point. \textit{The decompressor
  has no need of a hash table to store the codes}. This makes sense as
we are really only using the number codes to look up strings in the
table to manage the string table. Since the codes are numbers, they
can be used as indices and hence there is no need for a hash table.

\todo{fix string format}
\begin{algorithm}[H]
  \caption{Translating a string code to normal string.}
  \label{alg:translate-string-code}
  \begin{algorithmic}[1]
    \Let{$\strpair{stringCode}{characterCode}$}{$code$}
    \Let{$decodedString$}{$""$}
    \While{\True}
      \Let{$decodedString$}{$decodedString + characterCode$}

      \If{$stringCode = -1$}
        \Break
      \Else
        \Let{$\strpair{stringCode}{characterCode}$}{$stringCode$}
      \EndIf
    \EndWhile
  \end{algorithmic}
\end{algorithm}

\subsection{Working algorithm}

Now we are to discuss the fully working LZW compression
algortithm. This algortihm is not really much different from the
non-working one. Because it is only of a very,very rare edge the the
last algorithm may fail to work. And now, without any further ado, the
fully working LZW compresson algorithm is presented at algortihm
\ref{alg:lzw-working-decompression}.

\begin{algorithm}[H]
  \caption{LZW non-working decompression algorithm.}
  \label{alg:lzw-working-decompression}
  \begin{algorithmic}[1]
    \Let{$oldCode$}{\VoidCall{$InputCode$}}
    \State \Call{WriteByte}{$oldCode$}
    \Let{$character$}{\VoidCall{$oldCode$}}

    \Let{$newCode$}{\VoidCall{$ReadByte$}}

    \While{$newCode \neq maxValue$}

      \If{$\NOT$ \Call{IsInTable}{$newCode$}}
        \Let{$string$}{\Call{tranlate}{oldCode}}
        \Let{$string$}{$string + character$}
      \Else
        \Let{$string$}{\Call{translate}{$newCode$}}
      \EndIf

      \State \Call{outputString}{$string$}

      \Let{$character$}{$string[0]$} \Comment{Get the first character
        of the translated string.}

      \State \Call{$AddToStringTable$}{$oldCode, character$}

      \Let{$oldCode$}{$newCode$}

      \Let{$newCode$}{\VoidCall{$ReadByte$}}

    \EndWhile
  \end{algorithmic}
\end{algorithm}

\section{Efficiency}

In this section we will be testing how good of a compression ratio the
\lzw algorithm have on different sorts of files and data with
different codes sizes.

We will not however be discussing about the \textit{performance} of
the algorithm. Read a more advanced book on data compression of you
want such information.

\subsection{The Canterbury Corpus}

As you may noticed, doing LZW compression on small data is a very small
gain operation. However, doing this on larger files do actually yield a
significant decrease in file sizes.

In the chapter, we introduce a set of test files we'll be using the
demonstrate the efficiency of compression algorithms. A corpus of
files for designed testing new compression algorithms called the
Canterbury Corpus is that we will be using. These files we're selected
out of thousands of other files because they demonstrate the
efficiency of compression algorithms the best \cite{arnold:corpus}.

Table \ref{tab:corp-files} summaries all the files we'll be using. They can all be
downloaded from \cite{powell:desc-corp}.

\todo{typesett the file names using typewriter font.}
\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    File & Category \\
    \midrule
    alice29.txt & English text \\
    asyoulik.txt & Shakespeare \\
    cp.html & HTML source \\
    fields.c & C source \\
    grammar.lsp & LISP source \\
    kennedy.xls & Excel Spreadsheet \\
    lcet10.txt & Technical writing \\
    plrabn12.txt & Poetry \\
    sum & SPARC Executable \\
    xargs.1 & GNU manual page \\
    E.coli & Complete genome of the E. Coli bacterium \\
    bible.txt & The King James version of the bible \\
    world192.txt & The CIA world fact book \\
    \bottomrule
  \end{tabular}
  \caption{}
  \label{tab:corp-files}
\end{table}

\subsection{The Test}

The test we will be doing in this section as that we will be testing
how the compression ratio vary with different code sizes and different
kinds of files. We'll running the test on all the files in table and
we'll doing it for every file with the code sizes in the range $9 \leq
codeSize \leq 15$

\subsection{The Test Results}

In table \ref{tab:corp-files} we show the results of the test.

\begin{table}
  \scriptsize
  \centering
  \input{../code/lzw/table.tex}
  \caption{LZW test results. The different percentages the represent the respective compression ratios of that code size.}
  \label{tab:corp-files}
\end{table}

\subsection{Discussion}

And now we are to discuses these test results, and see if they make us
realist some new things about the \lzw algorithm.

The first thing we notice is that more is not always better. And by
that I am referring to the code sizes. Bigger code sizes does
\textit{not} always result in better compression. But what we should
also realise is the fact that codes with bigger sizes also take up
more space, than smaller codes. See for example the files
\verb|fields.c| and \verb|grammar.lsp|, these are source and/or text
files. What is peculiar about these files is that for highest code
size, $15$, they do not achieve the highest compression. What is even
stranger is that these files are program source code, meaning that are
highly likely chok of redundancies.

But we should also realize that these are not very large, they are
some of the smallest files. So it highly likely that is because the
15-bit codes are not really used. 15-bit codes means that the string
table has a maximum size of $2^{15} = 32768$, which is nowhere near
the sizes of these files. And thus, the conclusion we reach is that
these files were compressed poorly using 15-bit codes because the
extra bit added to 14-bit codes was not even used. The files were just
to small to even use and the compression of the file was over before
the string table was even full.

And true enough, if we inspect the larger files, say \verb|bible.txt|
or \verb|world192.txt|, we see that larger code sizes do indeed mean
greater compression. But this only because these files were large
enough to actually make use of the extra bits the larger code sizes
added.

\section{Sample code}

The sample source code of this chapter can found in the directory
\path|code/lzw|. Here the program \texttt{lzw} can found, which was
used in the previous tests. It allows you to perform \lzw compression
on files and even vary the sizes the codes.

\FloatBarrier

\printbibliography[heading=subbibliography]

\end{refsection}
