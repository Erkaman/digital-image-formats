\begin{comment}
  \bibliography{project.bib}
\end{comment}

\chapter{LZW}
\label{cha:lzw}

In this chapter, we will discuss the data compression algorithm \lzw,
as it is defined and described in
\cite{Nelson:1989:LDC:77102.77104,Welch:1984:THD:1319729.1320134,Salomon:2004:DCC,mark1996data_compression_book,nelson:_lzw_revis}. We
will first discuss the history behind the algorithm, then the
compression algorithm itself, after this we will describe the
decompression algorithm and in the end we will discuss the compression
efficiency of this algorithm.

\section{History}
\label{sec:hist-lzw}

\newcommand*{\lzonev}{{\acronymstyle lz\oldstylenums{1}}\xspace}
\newcommand*{\lztwov}{{\acronymstyle lz\oldstylenums{2}}\xspace}

In the years 1977 to 1978, the two Israeli researchers Abraham Lempel
and Jacob Ziv published two papers; in the first of these papers,
\cite{Ziv77auniversal}, they described the algorithm that came to be
known as \lzone(sometimes also known as \lzonev) and in the second of
these papers, \cite{Ziv78compressionof}, they defined the algorithm
that would from then on become known as \lztwo(also known as \lztwov)
\cite{roelofs09:_histor_portab_networ_graph_png_format,Salomon:2004:DCC,winters:_us_paten_adapt}.

These two methods started the family of compression algorithms that is
known as the \textit{dictionary methods}. Examples of algorithms in
the family of dictionary methods is \lzma, which is based on \lztwo
\cite{palov11}, \lzw, which is also based on \lztwo, and \lzss,which
was based on \lzone \cite{Salomon:2004:DCC}. \lzw is the method that
we will discuss in this chapter. It was invented by Terry Welch and he
published the algorithm in \cite{welch85:_u}.

\section{The Compression algorithm}
\label{sec:lzw-comp-desc}

The \lzw compression algorithm is best
understood by walking through the \lzw algorithm's process of
compressing some example string, so we will in this example
demonstrate how the compression of the string
$\var{ababcbababaaaaaaa}$ is done in \lzw.

Central to the entire algorithm's process is a string table(also
sometimes referred to as a dictionary). In this table, numbers called
\textit{codes} are assigned to strings. The algorithm starts by
filling this string table with all the strings that are possible to
fit in a byte. So all the codes $0-255$ are assigned their respective
strings. We will for sake of simplicity use ASCII encoding throughout
this example, so the codes $0-255$ are assigned their respective ASCII
characters, as is shown in table \ref{tab:str-tab-ascii}.

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    Code & String\\
    \midrule
    \dots & \dots \\
    $33$ & $!$ \\
    $34$ & $"$ \\
    $35$ & $\#$ \\
    \dots & \dots \\
    $97$ & $a$ \\
    $98$ & $b$ \\
    $99$ & $c$ \\
    $100$ & $d$ \\
    $101$ & $e$ \\
    \dots & \dots \\
    $255$ & unassigned to in ASCII \\
    \bottomrule
  \end{tabular}
  \caption{The initial \lzw string table}
  \label{tab:str-tab-ascii}
\end{table}

Now the actual compression of the string starts. First it reads in the
character $a$. In the string table that letter has the code $97$,
so the code $97$ is outputted. Next, the character $b$ is read,
which has the code $98$. So $98$ is outputted and then the two strings
read so far are appended together to form the string $\var{ab}$. The
string table is searched for the string $\var{ab}$, but it cannot be
found in the string table, so it is added to the string table and is
assigned the next available code, $256$.

$a$ is discarded, $b$ is kept and another $a$ is
read. First $b$ is outputted, and the appended string
$\var{ba}$ is formed. This string does neither exist in the table,
 so it is added to table and is given the code $257$.

 Discard $b$, output $a$ and read in another $b$. The
 resulting appended string is this time $\var{ab}$. But wait a minute,
 is not already that string in the string table? Yes it is, so instead
 of re-adding that string to table, it is kept for the next step in
 the algorithm.

The code for $\var{ab}$ is outputted, which is $256$, and the character $c$ is
read. The string $\var{abc}$ does not exist in the table, so it is
added to the string table and given the code $258$.

And the algorithm just keeps going on like this. Table
\ref{tab:str-tab-str} shows all the codes that were added during the
compression of the sample string $\var{ababcbababaaaaaaa}$. Table
\ref{tab:lzw-walkthru} gives a detailed walkthrough of the entire
compression process of that same string. So the string
$\var{ababcbababaaaaaaa}$ was in the end compressed down to the
string $\var{ab},256,c,257,260,a,262,263,a$.

So to summarize, the \lzw algorithm basically works like: the algorithm
keeps accumulating and reading characters into the string $S$. This
process goes on as long as, for the read in character $c$, the string
$Sc$, where $Sc$ represents the string that is formed when the
character $c$ is appended to the end of the string $S$, can be found
in the dictionary. When $Sc$, for some read in character $c$, cannot
be found in the dictionary, then the code for the string $S$ is
outputted, and the appended string $Sc$ is added to the dictionary.

\begin{Exercise}[label={lzw-compress}]

  \lzw compress the string $\var{abababab}$. Show the output of the
  compressor and the resulting string table.

\end{Exercise}

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    String & Code \\
    \midrule
    $97$ & $a$ \\
    $98$ & $b$ \\
    $99$ & $c$ \\
    \dots & \dots \\
    $256$ & $\var{ab}$ \\
    $257$ & $\var{ba}$ \\
    $258$ & $\var{abc}$ \\
    $259$ & $\var{cb}$ \\
    $260$ & $\var{bab}$ \\
    $261$ & $\var{baba}$ \\
    $262$ & $\var{aa}$ \\
    $263$ & $\var{aaa}$ \\
    $264$ & $\var{aaaa}$ \\
    \bottomrule
  \end{tabular}
  \caption{The string table after compression}
  \label{tab:str-tab-str}
\end{table}

\newcommand{\lzwrow}[6]{$\var{#1}$ = $#2$ & $\var{#3}$ = $#4$ &
  $#1$ = $#2$ & $\var{#5}$ = $#6$ \\}

\newcommand{\stoplzwrow}[2]{$\var{#1}$ = $#2$ & STOP &
  $\var{#1}$ = $#2$ & STOP \\}

\begin{table}
  \centering
  \begin{tabular}{llll}
    \toprule
    String Code & Character Code & Output Code & New table entry \\
    \midrule
    \lzwrow{a}{97}{b}{98}{ab}{256}
    \lzwrow{b}{98}{a}{97}{ba}{257}
    \lzwrow{ab}{256}{c}{99}{abc}{258}
    \lzwrow{c}{99}{b}{98}{cb}{259}
    \lzwrow{ba}{257}{b}{98}{bab}{260}
    \lzwrow{bab}{260}{a}{97}{baba}{261}
    \lzwrow{a}{97}{a}{97}{aa}{262}
    \lzwrow{aa}{262}{a}{97}{aaa}{263}
    \lzwrow{aaa}{263}{a}{97}{aaaa}{264}
    \stoplzwrow{a}{97}
    \bottomrule
  \end{tabular}
  \caption{Detailed \lzw compression process}
  \label{tab:lzw-walkthru}
\end{table}

\section{Implementation of The Compression Algorithm}
\label{sec:lzw-enc-algorithm}

In algorithm \ref{alg:lzw-compression} the complete \lzw compression
algorithm is presented. Let us now discuss the details of it.

\begin{algorithm}[H]
  \caption{\lzw compression algorithm}
  \label{alg:lzw-compression}
  \begin{algorithmic}[1]

    \Let{$\var{maxValue}$}{$2^{\var{codeSize}} - 1$}
    \Let{$maxCode$}{$\var{maxValue} - 1$}

    \State Fill the string table for the codes $1-255$
    \Let{$\var{nextCode}$}{$256$} \Comment{The code value the next string
      will be assigned to.}
    \Let{$\var{string}$}{\VoidCall{$ReadByte$}}
    \Let{$\var{character}$}{\VoidCall{$ReadByte$}}

    \While{\neof}

      \If{\Call{InStringTable}{$\var{string} + \var{character}$}} \label{algl:hasingcheckintable}
        \linecomment{Keep accumulating the string}
        \Let{$\var{string}$}{$\var{string} + \var{character}$}

      \Else

        \State \Call{outputCode}{$\var{string}$}

        \linecomment{Only add the accumulated string to the table if
          there is space for it.}  \If{$\var{nextCode} \leq maxCode$}

          \State \Call{AddToStringTable}{$\var{nextCode}, \var{string} + \var{character}$}\label{algl:hashadd}
          \Inc{\var{nextCode}}{1}

        \EndIf

        \Let{$\var{string}$}{$\var{character}$}

      \EndIf

      \Let{$\var{character}$}{\VoidCall{$ReadByte$}}

    \EndWhile

    \State \Call{outputCode}{$\var{string}$}
    \State \Call{outputCode}{$\var{maxValue}$}
  \end{algorithmic}
\end{algorithm}

\subsection{Codes sizes}

A \lzw compressed file is just a long sequence of outputted codes.
Since all the of the string codes added during the compression step
will have code values over $255$, bytes will obviously not be
sufficient to store these codes. In our version of the \lzw algorithm,
fixed code sizes are assigned to every code. This means that the codes
are stored in fixed $n$-bit numbers for some value $n$. The most
typically used code sizes are in the range $9 \leq n \leq 15$.

This is of course not the only possible storage model for the
codes. In the GIF format's variation of the \lzw algorithm, increasing
codes sizes are used to even further improve the compression
efficiency of this algorithm. We will discuss this variation in chapter
\ref{cha:gif}.

\subsection{Knowing when to stop}

The decompressor has to be notified of when it is done decompressing
the data. The largest possible value of a valid code size is therefore
outputted in the end of the algorithm. Once the decompressor finds
this value, it will know that it is done in decompressing the data.

\subsection{Table size}

Since we are using fixed code sizes there must be a limit on how large
the string table can grow. For any fixed code size $n$, this code size
can represent $2^n$ different codes. However, since the largest
possible code is used to signify the end of the compressed data, the
maximum length of the string table will be $2^n - 1$ rather than $2^n$

\section{Description of the Decompression algorithm}

Using the compression algorithm \lzw on the string
$\var{ababcbababaaaaaaa}$ produced the compressed data $ab, 256,c,
257, 260, a, 262, 263, 264, a,4095$. Since we were using 12-bit codes
to encode the data, the number $2^{12} - 1 = 4095$ terminates the
data. This number is used by the decompressor to check if it is done
with the decompression.

In a \lzw compressed file the string table used to encode the file is
not included. But how could you possibly decompress the file without
such essential information? It turns out that only from the compressed
data the decompressor is able to rebuild an exact copy of the string
table built during the compression.

We wanted to decompress $ab, 256,c, 257, 260, a, 262, 263, 264,
a,4095$. The decompressor first assign the code values $0-255$ to
their corresponding single length strings, and makes the same table
\ref{tab:str-tab-ascii} that was also built in the beginning of the
compression.

The characters $a$ and $b$ are read and $a$ is outputted. Now $a$ and
$b$ are appended together to form the string $\var{ab}$. This string is
added to the string table and is given the code $256$.

A new code $256$ is read, $a$ is discarded and $b$ is outputted. The
code $256$ is according to our current string table the string
$\var{ab}$.

Now we need to form the next string to be added to the table. To do
this, we take the string $b$ and append it to the first character of
the string $\var{ab}$, thus forming the string $\var{ba}$. This string is added to
the table with the code $257$.

The character is read $c$, the old $b$ is discarded and the string
$\var{ab}$ is now outputted. $\var{ab}$ and $c$ are appended together
to form the string $\var{abc}$(The first character of the one length
string $c$ is simply $c$) and then $\var{abc}$ is added to the string
table with the code $258$.

The code $257$ is read in, which corresponds to the string
$\var{ba}$. Since the former code was $c$, the string to be added to
the table is $\var{cb}$, and this string is given the code $259$.

Next the code $260$ is read. But we have not even yet added that code
to the table! How could this be?

Up to this point, the compressor had compressed the string
$\var{ababc}$ and had defined the code for the string $\var{ba}$ as
$257$. What remained to be compressed after this was
$\var{bababaaaaaaa}$. So it read the string $\var{ba}$ and the
character $b$, becuase $\var{ba}$ was already defined in the string
table. After outputting the code for $\var{ba}$, it added the string
$\var{ba}b$ to string the string table, assigning it the code
$260$. Then it threw away $\var{ba}$ and kept $b$. Then it accumulated
the string $\var{bab}$ until it found the character $a$. At this
point, the code $260$ for the string $\var{bab}$ was outputted.  The
important thing to realize here is that this happened before the
decompressor even had the chance the define the code $260$.

\newcommand{\ko}{\ensuremath{\var{k\omega}}\xspace}
\newcommand{\kok}{\ensuremath{\var{\ko k}}\xspace}
\newcommand{\kokok}{\ensuremath{\var{\kok \omega k}}\xspace}

So the general problem could described like this\cite{welch85:_u}: if
the string \ko is already in the string table and if the string \kokok
is encountered, the compressor will first add \kok the string table,
and then output \ko. It will after this accumulate and output the code
for the string \kok, and this is before the decompressor has defined
that code in its own string table. However, since this is the only
special case in which a undefined code can legally be found, this
special case can be handled by instead defining the current string to
be $\kok$. This string is equivalent to the string represented by the
undefined code.

Now let us return to our example string. The codes we currently had at
hand were $ba$ and $260$. We want to get the characters $k$ and
$\omega$, so that we may construct the string $\kok$. Since $ba$ are
the first two letters of the string $\kokok$, then $k = b$ and $\omega
= a$, so the string represented by the $260$ is $\kok = bab$.

And the decompressor just keeps working through the data like this,
while at the same time reconstructing the string table that was used
to compress the data in the first place. This just keeps on going
until it encounters the maximum value of the current code size $n$,
$2^{n}-1$. For 12-bit codes this value is $2^{12} - 1 = 4095$, as it
can be seen from our example data $97, 98, 256, 99, 257, 260, 97, 262,
263, 264, 4095$.

\begin{Exercise}[label={lzw-decompress}]

  Assuming that 12-bit codes were in the compression of exercise
  \ref{lzw-compress}, then compress the resulting data
  $97,98,256,258,98,4095$.

\end{Exercise}

In \ref{alg:lzw-working-decompression} pseudocode of the algorithm we
just described is given.

\begin{algorithm}[H]
  \caption{\lzw decompression algorithm}\algohack{}
  \label{alg:lzw-working-decompression}
  \begin{algorithmic}[1]

    \State \VoidCall{FillInitialStringTable}
    \Let{$\var{nextCode}$}{$256$}

    \Let{$\var{oldCode}$}{\VoidCall{$InputCode$}}
    \State \Call{writeByte}{$\var{oldCode}$}

    \Let{$\var{character}$}{$\var{oldCode}$}

    \Let{$\var{newCode}$}{\VoidCall{$InputCode$}}

    \While{$\var{newCode} \neq \var{maxValue}$}

      \If{$\NOT$ \Call{IsInTable}{$\var{newCode}$}} \Comment{The special case}
        \Let{$string$}{\Call{TranslateCodeToString}{$\var{oldCode}$}}
        \Let{$string$}{$string + \var{character}$}
      \Else
        \Let{$string$}{\Call{TranslateCodeToString}{$\var{newCode}$}}
      \EndIf

      \State \Call{outputString}{$string$}

      \Let{$\var{character}$}{$string[0]$} \Comment{Get the first character
        of the string.}

      \If{$\var{nextCode} \leq maxCode$}
        \linecomment{Add the translation of $\var{oldCode}$ $+$ $\var{character}$
          to the table}
        \State \Call{$AddToStringTable$}{$\var{nextCode},\var{oldCode}, \var{character}$}

        \Inc{\var{nextCode}}{1}
      \EndIf

      \Let{$\var{oldCode}$}{$\var{newCode}$}

      \Let{$\var{newCode}$}{\VoidCall{$ReadByte$}}

    \EndWhile
  \end{algorithmic}
\end{algorithm}

\section{Compression Efficiency Of LZW}

In this section, we will be showing and discussing the results of a
test that tested how the compression ratio of the \lzw algorithm varied
for different sorts of files and data with different codes sizes.

\subsection{How compression is achieved in LZW}

The main idea behind \lzw compression is that strings that occur often
within a file will get replaced by shorter codes. And in general, the
longer the algorithm runs, the longer the strings that get added to
the table get. This has the consequence that larger files typically
have a much better compression ratio than smaller files.

\subsection{The Canterbury Corpus}

Here we introduce a set of test files that we will be using to test
the efficiency of compression algorithms. A corpus of files designed
for testing new compression algorithms called the Canterbury Corpus is
that we will be using. These files were selected by
\cite{arnold:corpus} out of thousands of other files because they were
shown to test the efficiency of lossless compression algorithms the
best. Table \ref{tab:corp-files} lists all the files in the Canterbury
Corpus. They can all be downloaded at \cite{powell:desc-corp}.

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    File & Category \\
    \midrule
    alice29.txt & English text \\
    asyoulik.txt & Shakespeare Play \\
    cp.html & HTML source \\
    fields.c & C source \\
    grammar.lsp & LISP source \\
    kennedy.xls & Excel Spreadsheet \\
    lcet10.txt & Technical writing \\
    plrabn12.txt & Poetry \\
    sum & SPARC Executable \\
    xargs.1 & GNU manual page \\
    \bottomrule

  \end{tabular}
  \caption{The Canterbury Corpus}
  \label{tab:corp-files}
\end{table}

\subsection{The Test}

The purpose of the test was to test for the compression ratio for all
the files in table \ref{tab:corp-files} for different code fixed sizes
$n$ in the range $9 \leq n \leq 15$.

The program that was used to do the compression was \verb|LZW|, which
also is the program that was written to demonstrate the techniques
discussed in this chapter. The source code for the program can be find
in \verb|code/lzw/|.

To run the run the program on all the test files and accumulate the
gathered data into a \LaTeX{} table, a Python\footnote{\url{http://python.org/}} script was written. This
script can be found at the location \verb|code/lzw/test.py|.

\subsection{The Test Results}

In table \ref{tab:lzw-test-results} we show the results of the test.

\begin{table}
  \scriptsize
  \centering
  \input{../code/lzw/table.tex}
  \caption{\lzw compression test results}
  \label{tab:lzw-test-results}
\end{table}

\subsection{Discussion}

Now we will discuss the results gathered in table
\ref{tab:lzw-test-results} and see if we can draw any sort of
conclusion from them.

\subsubsection{Human Readable Text}

We will first discuss the four files \verb|asyoulik.txt|,
\verb|alice29.txt|, \verb|plrabn12.txt|, \verb|lcet10.txt|. What these
files have in common is that they all contain only human-readable
English text. Interestingly enough, if we exclude the file
\verb|asyoulik.txt|, they all seem to share a common pattern: larger
codes sizes means better compression(the lower compression ratio, the
better compression). This makes sense because the English language,
and human languages in general, is full of redundancies. So for larger
code sizes, larger string tables can be built and therefore a greater
number of redundancies can be eliminated from the text.

You may also notice that the largest files, \verb|plrabn12.txt| and
\verb|lcet10.txt|, achieved the best compression. This is because for
longer files much more of the string table will get built up. The
longer a compression goes on, the larger the string table gets. And
the larger the string table gets, the better the data can be
compressed.

For the smallest file, \verb|asyoulik.txt|, the best compression ratio
was not achieved for the largest code size. This is because for
smaller files it is unlikely that the string table even gets fully
built up. If the size of the code does not get used to its full
potential, then unnecessary bits are only added when the code size is
increased. These unnecessary bits has the consequence that the codes
are stored in larger numbers, and this in turn can unnecessarily
increase the size and worsen the compression.

\subsubsection{Program code}

For the two source code files, \verb|fields.c| and \verb|grammar.lsp|,
the compression ratio is not get very good at all, compared to the
compression ratio of the text files. This is surprising, because
source code tends to contain tons of keywords, like
\verb|for|,\verb|while| \verb|return| that are repeated throughout the
entire file. You would think that by replacing all these repeated words by
small codes this would result a good compression ratio.

But the worse compression ratio for higher codes sizes is in this case
attributed to the very low size of the files. Since these files are so
small, it is very hard for them to get a better compression ratio than
that of the huge English text files.

\subsubsection{Both text and code}

The files \verb|cp.html| and \verb|xargs.1| could be said to contain a
combination of both English human-readable text and source
code. \verb|xargs.1| is a pretty small file and hence gets its maximum
compression ratio for the rather small code size of $11$. The file
\verb|cp.html| is bit larger than the former, but the compression
ratio for this file behaves in largely the same way.

\subsubsection{Binary formats}

The files \verb|sum| and \verb|kennedy.xls| are files in binary
format. That is to say, they consist of a sequence of numbers
unreadable by humans. Binary files can most often only be interpreted
by the applications that created them. Image formats are examples of
binary formats.

The file \verb|kennedy.xls| achieves the best possible compression
ratio in this entire test. This is mainly because this is the largest
file in the entire Canterbury Corpus.

\verb|sum| is a binary file that has very unstable and unpredictable
compression ratios. But it in general seems to achieve the better
ratios using the larger codes.

\subsection{Conclusion}

The main conclusion of these tests is that for larger files larger
code sizes also tend to result in better compression ratios. For
smaller files larger codes can on the other hand unnecessarily worsen
the compression ratio.

\FloatBarrier

\section{Answers The Exercises}

\begin{Answer}[ref={lzw-compress}]

%\texttt{abababab}

  \begin{center}
    \begin{tabular}{llll}
      \toprule
      String Code & Character Code & Output Code & New table entry \\
      \midrule

      \lzwrow{a}{97}{b}{98}{ab}{256}
      \lzwrow{b}{98}{a}{97}{ba}{257}
      \lzwrow{ab}{256}{a}{97}{aba}{258}
      \lzwrow{aba}{258}{b}{98}{abab}{259}
      \stoplzwrow{b}{98}
      \bottomrule
   \end{tabular}


  \end{center}

  So the string is compressed down to $97,98,256,258,98$.
\end{Answer}

\begin{Answer}[ref={lzw-decompress}]

  The decompressed data you should get is $\var{abababab}$.

\end{Answer}