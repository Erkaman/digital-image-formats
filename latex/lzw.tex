\begin{comment}
  \bibliography{project.bib}
\end{comment}

\chapter{LZW}
\label{ch:rle}

\begin{refsection}

  In this chapter, we will discuss the data compression algorithm
  \lzw\index{LZW} as it is described in
  \cite{nelson89:_lzw_data_compr,Welch:1984:THD:1319729.1320134,Salomon:2004:DCC}. We
  will first discuss the history behind the algorithm, then
  compression algorithm\index{compression algorithm}, after this the
  \textit{de}compression algorithm and in the end we will discuss the
  efficiency and compression ratio of this algorithm.

\section{(short)History}

\lzw stands for \textbf{L}empel, \textbf{Z}iv and
\textbf{W}elch. Lempel and Ziv are the two researches who developed
the two compression methods \lzseven and \lzeight. These two
algorithms became the basis for many other \lz variations including
\lzma \cite{palov11}, \lzw and \lzss \cite{Salomon:2004:DCC}. But we
will only discuss \lzw in this chapter. And finally, Welch is the guy
the developed this \lz variation.

\section{The Compression algorithm}

\subsection{Description}
\label{sec:lzw-desc}

The \lzw compression\index{\lzw compression} algorithm is best
understood by walking through the LZW algorithm's process of
compressing some sample data.

The algorithm is going to compress the sample data
\texttt{ababcbababaaaaaaa}.

Central to entire algorithm's process is a string table. In this
table, number called \textit{code}\index{code} is assigned to a
string. The algorithm start by filling the string table with all the
strings that are possible to fit in a byte. That is, it adds all the
possible number codes 0-255 to the string table and these number codes
are assigned a one length string that is the character that it
represents in the current encoding.. For this example we will use the
character set \ascii \cite{rfc20}, but really any character set could
be used. But in fact it is not really necessary that character set is
used at all, because all these strings are supposed to represent

Table \ref{tab:str-tab-ascii} shows how this table ends up looking
like when it is filled.

\newcommand{\dotsrow}{\dots & \dots \\}
\newcommand{\strrow}[2]{$#1$ & #2 \\}

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    Code & String\\
    \midrule

    \dotsrow
    \strrow{33}{!}
    \strrow{34}{"}
    \strrow{35}{\#}
    \dotsrow
    \strrow{97}{a}
    \strrow{98}{b}
    \strrow{99}{c}
    \strrow{100}{d}
    \strrow{101}{e}
    \dotsrow
    \strrow{255}{}

    \bottomrule
  \end{tabular}
  \caption{The initial LZW string table, filled with all the \ascii
    characters.}
  \label{tab:str-tab-ascii}
\end{table}

%\newcommand*{\newcode}[1]{%
%  \expandafter\newcommand\csname#1\endcsname c[1][]{\texttt{#1}##1\xspace}}

%\newcode{a}
\newcommand{\acode}{\texttt{a}\xspace}
\newcommand{\bcode}{\texttt{b}\xspace}
\newcommand{\ccode}{\texttt{c}\xspace}
\newcommand{\abcode}{\texttt{ab}\xspace}
\newcommand{\bacode}{\texttt{ba}\xspace}
\newcommand{\abccode}{\texttt{abc}\xspace}

Now the algorithm actually start going through the sample
string. First it reads in the character \acode. In the string table
that letter has the code $97$, so the code $97$ is outputted. Next,
the character \bcode is read, which has the code $98$. So $98$ is
outputted and then the two strings read so far are appended together
to form the string \abcode. The string table is searched for the
string \abcode, but it cannot be found in the string table, so it is
added to the string table and is assigned the next available code, $256$.

\acode is discarded, \bcode is kept and another \acode is
read. First \bcode is outputted, and the appended string
\bacode is formed. This string doesn't either exist in the table,
 so it is added to table and is given the code $257$.

Discard \bcode, output \acode and read in another
\bcode. Now something very interesting happens; the
resulting appended string is this time \abcode. But wait a minute,
isn't already that string in the string table? Yes it is, so instead
of re-adding that string to table, it is kept for the next step in the
algorithm.

The code for \abcode is outputted, which is $256$, and the character \ccode is
read. The string \abccode doesn't exist in the table, so it is
added to the string table and given the code $258$.

And the algorithm just keeps going like this. Table
\ref{tab:str-tab-str} shows all the codes that were added during the
compression of the sample string. Table \ref{tab:lzw-walkthru} gives a
detailed walkthrough of the compression of the same string. Do study
both of these tables table carefully and don't continue reading until you
fully understand what happened during the compression of the
string.

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    String & Code \\
    \midrule

    \dotsrow
    \strrow{256}{ab}
    \strrow{257}{ba}
    \strrow{258}{abc}
    \strrow{259}{cb}
    \strrow{260}{bab}
    \strrow{261}{baba}
    \strrow{262}{aa}
    \strrow{263}{aaa}
    \strrow{264}{aaaa}
    \bottomrule
  \end{tabular}
  \caption{The string table after the \lzw algorithm has been run on
    the string \texttt{ababcbababaaaaaaa}.}
  \label{tab:str-tab-str}
\end{table}

\begin{table}
  \centering
  \newcommand{\lzwrow}[4]{#1 & #2 & #3 & #4 \\}
  \begin{tabular}{llll}
    \toprule
    String Code & New Code & Output Code & New table entry \\
    \midrule

    \lzwrow{a}{b}{a = 97}{ab = 256}
    \lzwrow{b}{a}{b = 98}{ba = 257}
    \lzwrow{a}{b}{}{}
    \lzwrow{ab}{c}{ab = 256}{abc = 258}
    \lzwrow{c}{b}{c = 99}{cb = 259}
    \lzwrow{b}{a}{}{}
    \lzwrow{ba}{b}{ba = 257}{bab = 260}
    \lzwrow{b}{a}{}{}
    \lzwrow{ba}{b}{}{}
    \lzwrow{bab}{a}{bab = 260}{baba = 260}
    \lzwrow{a}{a}{a = 97}{aa = 261}
    \lzwrow{a}{a}{}{}
    \lzwrow{aa}{a}{aa = 262}{aaa = 262}
    \lzwrow{a}{a}{}{}
    \lzwrow{aa}{a}{}{}
    \lzwrow{aaa}{a}{aaa = 263}{aaaa = 263}

    \bottomrule
  \end{tabular}
  \caption{Detailed \lzw compression on the string \texttt{ababcbababaaaaaaa}.}
  \label{tab:lzw-walkthru}
\end{table}

\subsection{Algorithm}
\label{sec:lzw-enc-algorithm}

In algorithm \ref{alg:lzw-compression} the complete LZW compression
algorithm is presented. Try reading through it at least once,
but I doubt you'd be able to understand all of it at this point. The
implementation of the algorithm turns out not to be as easy as it
sounds.

\begin{algorithm}[H]
  \caption{The LZW compression algorithm.}
  \label{alg:lzw-compression}
  \begin{algorithmic}[1]
    \Let{$nextCode$}{$256$}
    \Let{$stringCode$}{\VoidCall{$ReadByte$}}
    \Let{$charCode$}{\VoidCall{$ReadByte$}}

    \While{\neof}

      \If{\Call{InStringTable}{$stringCode,charCode$}} \label{algl:hasingcheckintable}
        \Let{$stringCode$}{\Call{GetCode}{$stringCode,charCode$}} \label{algl:hasgetcode}
      \Else
        \State \Call{outputCode}{$stringCode$}

        \If{$nextCode \leq maxCode$}

          \State \Call{AddToStringTable}{$nextCode, stringCode, charCode$}\label{algl:hashadd}
          \Let{$nextCode$}{$nextCode + 1$}

        \EndIf

        \Let{$stringCode$}{$charCode$}

      \EndIf

      \Let{$charCode$}{\VoidCall{$ReadByte$}}

    \EndWhile

    \State \Call{outputCode}{$stringCode$}
    \State \Call{outputCode}{$maxValue$}

  \end{algorithmic}
\end{algorithm}

\subsubsection{Codes sizes}

All a \lzw compressed file will consist of after this algorithm has
been run, is just a long sequence of outputted codes.

Since all the of the actual strings(whose length is greater than one)
in the string table have codes values over $255$, bytes will obviously
not be sufficient to store these codes. The code sizes\index{code
  size} used in the \lzw algorithm are typically in the ranges $9 \leq
codeSize \leq 15$. But in our examples we will be using 12-bit codes,
as they are most commonly used.

File I/O Functions for writing data to files are not typically
designed to write numbers whose bit-count are not multiples of 8, so
we will need to construct a function, an algorithm, for doing this. It
should at least support writing numbers in the ranges  $9 \leq x \leq 15$.

\todo{discuss the function outputCode here?}

\subsubsection{String Storage Model}

\newcommand{\strpair}[2]{(#1,#2)}

You could of course just store all the strings in the string table as
real strings. However, these strings are going to end up
taking way more space than they really need to.

As you probably remember from section \ref{sec:lzw-desc}, the string
to be added to the table is just the concatenation of the character
just read and the former string. The concatenation of the characters
characters \texttt{a} and \texttt{b} could be expressed as the string
\texttt{ab}. But however, they could also be described by the
pair\index{pair} \strpair{97}{98}, which consists of the two codes of
the two strings. Okay, there is no real big difference in size yet,
but now consider \texttt{abcbde} and \texttt{a} and their
concatenation \texttt{abcbdea}. If \texttt{abcbde} has the code, say,
289, then surely the pair \strpair{289}{97} is much more space
efficent than \texttt{abcbdea}?  Yes it is, of course!

You may think we are just nitpicking here, but also need to consider
that with huge files this string table could end up becoming huge. And
besides, pair of numbers are often much easier to deal with than
strings, espically in languages like \C!

Using this new string storage model table \ref{tab:str-tab-str}, is
translated to table \ref{tab:str-tab-pair}. You also may notice that we end
creating this beautiful recursive structure using this model. We will
find out more about this later when are going to decompress the data.

\newcommand{\pairrow}[3]{$#1$ & \strpair{#2}{#3} \\}

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    String & Code \\
    \midrule
    \dotsrow
    \pairrow{256}{97}{b}
    \pairrow{257}{98}{a}
    \pairrow{258}{256}{c}
    \pairrow{259}{99}{b}
    \pairrow{260}{257}{b}
    \pairrow{261}{260}{a}
    \pairrow{262}{87}{a}
    \pairrow{263}{262}{a}
    \pairrow{264}{263}{a}
    \bottomrule
  \end{tabular}
  \caption{The string table after the \lzw algorithm has been run.}
  \label{tab:str-tab-pair}
\end{table}

\subsubsection{Table size}

Since we are using fixed code sizes there is a limit on how large the
string table can grow. For example, for 12-bit codes the maximum size
of the string table will be $2^{11}=4096$. In algorithm
\ref{alg:lzw-compression} the constants $maxCode$ and $maxValue$ are
used to be make sure that the table doesn't grow beyond it's maximum
size. In algorithm \ref{alg:lzw-constants} we show how set the size
constants for the example code size 12. Notice that the maximum value
of the final code, $maxCode$ is one less than the actual max size of
the table. That's because at the end of algorithm we output the final
code to always be the maximum value of the current code size. You will
soon see why we do this when discuss the decompression algorithm, but for
now ignore it.

\begin{algorithm}[H]
  \caption{Settings the constants for the LZW algorithm for the
    example code size 12.}
  \label{alg:lzw-constants}
  \begin{algorithmic}[1]
    \Let{$codeSize$}{12}
    \Let{$maxValue$}{$(1 << codeSize) - 1$}
    \Let{$maxCode$}{$maxValue - 1$}
  \end{algorithmic}
\end{algorithm}

\subsubsection{Managing The String Table}

Please Inspect line
\algref{alg:lzw-compression}{algl:hasingcheckintable} and line
\algref{alg:lzw-compression}{algl:hasgetcode} of the compression
algorithm. Since we are clearly using the strings themselves to search
for their corresponding codes, we need to figure out how to find their
respective codes.

One way to implement this would be by using the code as an index to an
array of strings. But this clearly prohibitively slow as you would
have search the entire string table, string by string, when you would
want the code of a specific string.

The easiest way to solve this problem is to just use a hash table. The
string can used to create a key to index the table to get the searched
codes.

Since most modern languages already implement some sort of hash table
in their standard library, this functionality should not be very hard
to implement. If you are using more lower level language like \C, you
will either have to roll your own or use a library. In the sample \C
source code of this chapter a simple hand-rolled hash table
is used to manage the string table.

\section{Decompression algorithm}

\subsection{Description}

Using the compression algorithm LZW on the string
\texttt{ababcbababaaaaaaa} produces the compressed data stream
\texttt{97 98 256 99 257 260 97 262 263 264 4095}. Since we were using
12-bit codes to encode the data, the maximum value of an unsigned
12-bit number,$2^{12} - 1 = 4095$, is used to terminate it. This is to
make sure that the decompressor knows when to stop reading.

As you may have noticed, in a LZW compressed file the string table is
not included. How could you possibly decompress the file without such
essential information? It turns out that all the LZW decompression
algorithm do is basically the same thing that the compression aglorithm do; it
just reconstructs the string table as it goes along and  reads and decompresses the
file at the same time.

So, we wanted to decompress the data stream \texttt{97 98 256 99 257
  260 97 262 263 264 4095}. The decompressor first makes the same
table \ref{tab:str-tab-ascii}  that the \lzw compressing algorithm did at the beginning of
its process.

First the first code is read simply and outputted, which is $98$, or
simply the letter \texttt{a}. And in the same fashion the next code
$98$ is read and outputted as \texttt{b}.

Now \texttt{a} and \texttt{b} are appended together to the string
\texttt{ab}. This string is added to the string table and is given the
code $256$.

A new code is read, this time it's $256$, and \texttt{a} is
discarded. $256$ is now to be translated. We just assigned the code
$256$ the string \texttt{ab}, so the translation of $256$ is obviously
\texttt{ab} and because of that \texttt{ab} is outputted.

Now we need to form the next string to be added the table,
\texttt{ba}. To do this, we take the former string \texttt{b} and add
to the first character of the current string \texttt{ab}, thus forming
the desired string \texttt{ba}. this string is added to the table with
the code $257$. And in this way all of the other strings to be added
to the table.

But we're going to keep doing this for a whole. You will soon see
why. $99$ is read and then is outputted as \texttt{c}. \texttt{ab} and
\texttt{c} are appended together to form the string \texttt{abc} and then
\texttt{abc} is added to the string table with the code $258$.

$257$ is read in. This is the code for \texttt{ba}.Since the former
code was \texttt{c}, the string to be added to the table is
\texttt{cb}, and it is given the code $260$.

But now we read in the code $260$ before we have even added it to the
table! Up to this we have so far uncompressed up to the string
\texttt{ababcba}. What remains is the string \texttt{babaaaaaaa}. The
problen here is that the compressor outputted a code before the
decompressor was able to define it. So we need to handle this as a
special case. We need to figure out how to construct the table entry
bab=260 with the information we currently got. We could actually
pretty easily construct this appending the old code \texttt{ba} to the
first character of the old code \texttt{b}.

\newcommand{\ko}{\ensuremath{k\omega}\xspace}
\newcommand{\kok}{\ensuremath{\ko k}\xspace}
\newcommand{\kokok}{\ensuremath{\kok \omega k}\xspace}

The general problem could described like: If the string \ko is already
in the string table and if the string \kokok is encountered, then
compressor will first add \kok the string table and then output
\ko. After the code for \kok is output before the decompressor is able
to define it. But this table entry can then be constructed using $\ko
+k$.  So this is really only a special case you need to handle when a
code is not defined in the string table.

So the decompressor just keeps working through the data like this while
deconstructing the string table that was used to compress it. It reads
until it encounters the maximum value of the current code size $x$,
$2^{x}-1$; for 12-bit codes this $2^{12} - 1 = 4095$, as can be seen
from out test data stream, \texttt{97 98 256 99 257 260 97 262 263 264
  4095}. The entire decompression process is shown in table \ref{tab:lzw-dec-walkthru}.

\begin{table}
  \centering
  \begin{tabular}{lllll}
    \toprule
    Old Code & Newcode Code & Input Code & Output Codes & New table entry \\
    \midrule

    97=a & & 97=a & a=97 & \\
    97=a & 98=b & 98=b & b=98 & ab=256 \\
    98=b & 256=ab & 256=ab & a=97,b=98 & ba=257 \\
    ab=256 & c=99 & c=99 & c=99  & abc=258 \\
    c=99 & 257=ba & 257=ba & b=98,a=97 & cb=259 \\
    bab & 260 & 260&b=98,a=97,b=98 &bab=260 \\
    bab=260 & a=97  & a=97 & a=97 & baba=261\\
    bab=260 & a=97 & a=97 & a=97 & baba=261\\

    aa & 262 & 262 & a=97,a=97 & aa=262 \\

    aaa & 263 & 263 & a=97,a=97,a=97 & aaa=263 \\

    aaa=263 & a=97 & a=97 & a=97 & aaaa=264  \\

    a=97 & 4095=maxValue & 4095=maxValue & STOP &  \\

    \bottomrule
  \end{tabular}
  \caption{Detailed \lzw decompression on the string \texttt{ababcbababaaaaaaa}.}
  \label{tab:lzw-dec-walkthru}
\end{table}

\subsection{Algorithm}

And the algorithm we have just described can be seen in algorithm
\ref{alg:lzw-non-working-decompression}. Having discussed the
compression algorithm very much in depth, you should be able to
perfectly understand most of this alglorithm. However, there are still
some more things we need to discuss until we are done with the \lzw
algorithm.

\begin{algorithm}[H]
  \caption{LZW non-working decompression algorithm.}
  \label{alg:lzw-working-decompression}
  \begin{algorithmic}[1]
    \Let{$oldCode$}{\VoidCall{$InputCode$}}
    \State \Call{WriteByte}{$oldCode$}
    \Let{$character$}{\VoidCall{$oldCode$}}

    \Let{$newCode$}{\VoidCall{$ReadByte$}}

    \While{$newCode \neq maxValue$}

      \If{$\NOT$ \Call{IsInTable}{$newCode$}} \Comment{The special case}
        \Let{$string$}{\Call{tranlate}{oldCode}}
        \Let{$string$}{$string + character$}
      \Else
        \Let{$string$}{\Call{translate}{$newCode$}}
      \EndIf

      \State \Call{outputString}{$string$}

      \Let{$character$}{$string[0]$} \Comment{Get the first character
        of the translated string.}

      \State \Call{$AddToStringTable$}{$oldCode, character$}

      \Let{$oldCode$}{$newCode$}

      \Let{$newCode$}{\VoidCall{$ReadByte$}}

    \EndWhile
  \end{algorithmic}
\end{algorithm}

\subsubsection{Translating string codes}

First we need to figure out how translate string from the code pair format as we
discussed in section \ref{sec:lzw-enc-algorithm}.

Please see algorithm \ref{alg:translate-string-code}. This puts the
decoded string of the code $code$ \textit{reversed} into the string
$decodedString$. The string ends up being reversed because we
basically are translating the code from the end of the string to the
beginning. As you can see, we are just working through character code
after character code looking up the next code in the table using the
string code. For this algorithm, we have chosen to put a negative one
into the stringCode of pairs that represent single characters. If we
have reached such a pair, we have reached to beginning of the string
and we are done translating it.

This discussion leads up to another point. \textit{The decompressor
  has no need of a hash table to manage the string table}. This makes sense as
we are really only using the number codes to look up strings in the
table to manage the string table. Since the codes are numbers, they
can be used as indices and hence there is no need for a hash table,
which makes the work for the compressor one heck of a lot easier.

\todo{fix string format}
\begin{algorithm}[H]
  \caption{Translating a string code to normal string.}
  \label{alg:translate-string-code}
  \begin{algorithmic}[1]
    \Let{$\strpair{stringCode}{characterCode}$}{$code$}
    \Let{$decodedString$}{$""$}
    \While{\True}
      \Let{$decodedString$}{$decodedString + characterCode$}

      \If{$stringCode = -1$}
        \Break
      \Else
        \Let{$\strpair{stringCode}{characterCode}$}{$stringCode$}
      \EndIf
    \EndWhile
  \end{algorithmic}
\end{algorithm}

\subsection{Working algorithm}

Now we are to discuss the fully working LZW compression
algortithm. This algortihm is not really much different from the
non-working one. Because it is only of a very,very rare edge the the
last algorithm may fail to work. And now, without any further ado, the
fully working LZW compresson algorithm is presented at algortihm
\ref{alg:lzw-working-decompression}.


\section{Efficiency}

In this section we will be testing how good of a compression ratio the
\lzw algorithm have on different sorts of files and data with
different codes sizes.

We will not however be discussing about the \textit{performance} of
the algorithm. Read a more advanced book on data compression of you
want such information.

\subsection{The Canterbury Corpus}

As you may noticed, doing LZW compression on small data is a very small
gain operation. However, doing this on larger files do actually yield a
significant decrease in file sizes.

In the chapter, we introduce a set of test files we'll be using the
demonstrate the efficiency of compression algorithms. A corpus of
files for designed testing new compression algorithms called the
Canterbury Corpus is that we will be using. These files we're selected
out of thousands of other files because they demonstrate the
efficiency of compression algorithms the best \cite{arnold:corpus}.

Table \ref{tab:corp-files} summaries all the files we'll be using. They can all be
downloaded from \cite{powell:desc-corp}.

\todo{typesett the file names using typewriter font.}
\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    File & Category \\
    \midrule
    alice29.txt & English text \\
    asyoulik.txt & Shakespeare \\
    cp.html & HTML source \\
    fields.c & C source \\
    grammar.lsp & LISP source \\
    kennedy.xls & Excel Spreadsheet \\
    lcet10.txt & Technical writing \\
    plrabn12.txt & Poetry \\
    sum & SPARC Executable \\
    xargs.1 & GNU manual page \\
    E.coli & Complete genome of the E. Coli bacterium \\
    bible.txt & The King James version of the bible \\
    world192.txt & The CIA world fact book \\
    \bottomrule
  \end{tabular}
  \caption{}
  \label{tab:corp-files}
\end{table}

\subsection{The Test}

The test we will be doing in this section as that we will be testing
how the compression ratio vary with different code sizes and different
kinds of files. We'll running the test on all the files in table and
we'll doing it for every file with the code sizes in the range $9 \leq
codeSize \leq 15$

\subsection{The Test Results}

In table \ref{tab:corp-files} we show the results of the test.

\begin{table}
  \scriptsize
  \centering
  \input{../code/lzw/table.tex}
  \caption{LZW test results. The different percentages the represent the respective compression ratios of that code size.}
  \label{tab:corp-files}
\end{table}

\subsection{Discussion}

And now we are to discuses these test results, and see if they make us
realist some new things about the \lzw algorithm.

The first thing we notice is that more is not always better. And by
that I am referring to the code sizes. Bigger code sizes does
\textit{not} always result in better compression. But what we should
also realise is the fact that codes with bigger sizes also take up
more space, than smaller codes. See for example the files
\verb|fields.c| and \verb|grammar.lsp|, these are source and/or text
files. What is peculiar about these files is that for highest code
size, $15$, they do not achieve the highest compression. What is even
stranger is that these files are program source code, meaning that are
highly likely chok of redundancies.

But we should also realize that these are not very large, they are
some of the smallest files. So it highly likely that is because the
15-bit codes are not really used. 15-bit codes means that the string
table has a maximum size of $2^{15} = 32768$, which is nowhere near
the sizes of these files. And thus, the conclusion we reach is that
these files were compressed poorly using 15-bit codes because the
extra bit added to 14-bit codes was not even used. The files were just
to small to even use and the compression of the file was over before
the string table was even full.

And true enough, if we inspect the larger files, say \verb|bible.txt|
or \verb|world192.txt|, we see that larger code sizes do indeed mean
greater compression. But this only because these files were large
enough to actually make use of the extra bits the larger code sizes
added.

\section{Sample code}

The sample source code of this chapter can found in the directory
\path|code/lzw|. Here the program \texttt{lzw} can found, which was
used in the previous tests. It allows you to perform \lzw compression
on files and even vary the sizes the codes.

\FloatBarrier

\printbibliography[heading=subbibliography]

\end{refsection}
